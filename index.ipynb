{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization Lab\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this lab, we'll gain experience detecting and dealing with a ANN model that is overfitting using various regularization and hyperparameter tuning techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "In this lab, we'll work with a large dataset of customer complaints to a bank, with the goal of predicting what product the customer is complaining about based on the text of their complaint.  There are 7 different possible products that we can predict, making this a multi-class classification task. \n",
    "\n",
    "\n",
    "#### Preprocessing our Data Set\n",
    "We'll start by preprocessing our dataset by tokenizing the complaints and limiting the number of words we consider to reduce dimensionality. \n",
    "\n",
    "#### Building our Tuning our Model\n",
    "Once we have preprocessed our data set, we'll build a model and explore the various ways that we can reduce overfitting using the following strategies:\n",
    "- Early stopping to minimize the discrepancy between train and test accuracy.\n",
    "- L1 and L2 regularization.\n",
    "- Dropout regularization.\n",
    "- Using more data.\n",
    "\n",
    "\n",
    "**_Let's Get Started!_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the Bank Complaints Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import the libraries and take a sample\n",
    "\n",
    "Run the cell below to import everything we'll need for this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the cell below, import our data into a DataFrame.  The data is currently stored in `Bank_complaints.csv`.\n",
    "Then, `.describe()` the dataset to get a feel for what we're working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.read_csv('Bank_complaints.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed things up during the development process (and also to give us the ability to see how adding more data affects our model performance), we're going to work with a sample of our dataset rather than the whole thing.  The entire dataset consists of 60,000 rows--we're going to build a model using only 10,000 items randomly sampled from this.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Get a random sample of `10000` items from our dataset (HINT: use the `df` object's `.sample()` method to make this easy)\n",
    "* Reset the indexes on these samples to `range(10000)`, so that the indices for our rows are sequential and make sense.\n",
    "* Store our labels, which are found in `\"Product\"`, in a different variable.\n",
    "* Store the data, found in `\"Consumer complaint narrative`, in the variable `complaints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df.Product\n",
    "complaints = df['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>A XXXX collection account placed on my credit ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Product                       Consumer complaint narrative\n",
       "9999  Credit reporting  A XXXX collection account placed on my credit ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenizing the Complaints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll only keep 2,000 most common words and use one-hot encoding to quickly vectorize our dataset from text into a format that a neural network can work with. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Create a `Tokenizer()` object, and set the `num_words` parameter to `2000`.\n",
    "* Call the tokenizer object's `fit_on_texts()` method and pass in our `complaints` variable we created above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.word_counts)\n",
    "# print(tokenizer.document_count)\n",
    "# print(tokenizer.word_index)\n",
    "# print(tokenizer.word_docs)\n",
    "# word_counts: A dictionary of words and their counts.\n",
    "# word_docs: A dictionary of words and how many documents each appeared in.\n",
    "# word_index: A dictionary of words and their uniquely assigned integers.\n",
    "# document_count:An integer count of the total number of documents that were used to fit the Tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create some text sequences by calling the `tokenizer` object's `.texts_to_sequences()` method and feeding in our `complaints` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(complaints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll convert our text data from text to a vectorized matrix.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Call the `tokenizer` object's `.texts_to_matrix` method, passing in our `complaints` variable, as well as setting the `mode` parameter equal to `'binary'`.\n",
    "* Store the tokenizer's `.word_index` in the appropriate variable.\n",
    "* Check the `np.shape()` of our `one_hot_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode = 'binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results) # Expected Results (10000, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 One-hot Encoding of the Products Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tokenized and encoded our text data, we still need to one-hot encode our label data.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "\n",
    "* Create a `LabelEncoder` object, which can found inside the `preprocessing` module.\n",
    "* `fit` the label encoder we just created to `product`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what classes our label encoder found.  Run the cell below to examine a list of classes that `product` contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bank account or service',\n",
       " 'Checking or savings account',\n",
       " 'Consumer Loan',\n",
       " 'Credit card',\n",
       " 'Credit reporting',\n",
       " 'Mortgage',\n",
       " 'Student loan']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " list(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll need to transform `product` into a numeric vector.  \n",
    "\n",
    "In the cell below, use the label encoder's `.transform` method on `product` to create an integer encoded version of our labels. \n",
    "\n",
    "Then, access `product_cat` to see an example of how the labels are now encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 6, 1, ..., 5, 6, 4])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_cat = le.transform(product)\n",
    "product_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to go from integer encoding to one-hot encoding.  Use the `to_categorical` method from keras to do this easily in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check the shape of our one-hot encoded labels to make sure everything worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(product_onehot) # Expected Output: (10000, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train - test split\n",
    "\n",
    "Now, we'll split our data into training and testing sets.  \n",
    "\n",
    "\n",
    "We'll accomplish this by generating a random list of 1500 different indices between 1 and 10000.  Then, we'll slice these rows and store them as our test set, and delete them from the training set (it's very important to remember to remove them from the training set!)\n",
    "\n",
    "Run the cell below to create a set of random indices for our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = random.sample(range(1,10000), 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now:\n",
    "\n",
    "* Slice the `test_index` rows from `one_hot_results` and store them in `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = one_hot_results[test_index]\n",
    "\n",
    "# This line returns a version of our one_hot_results that has every item with an index in test_index removed\n",
    "train = np.delete(one_hot_results, test_index, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll need to repeat the splitting process on our labels, making sure that we use the same indices we used to split our data. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Slice `test_index` from `product_onehot`\n",
    "* Use `np.delete` to remove `test_index` items from `product_onehot` (the syntax is exactly the same above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot,test_index,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine the shape everything we just did to make sure that the dimensions match up.  \n",
    "\n",
    "In the cell below, use `np.shape` to check the shape of:\n",
    "\n",
    "* `label_test`\n",
    "* `label_train`\n",
    "* `test`\n",
    "* `train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 7)\n",
      "(8500, 7)\n",
      "(1500, 2000)\n",
      "(8500, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(label_test)) # Expected Output: (1500, 7)\n",
    "print(np.shape(label_train)) # Expected Output: (8500, 7)\n",
    "print(np.shape(test)) # Expected Output: (1500, 2000)\n",
    "print(np.shape(train)) # Expected Output: (8500, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`.\n",
    "\n",
    "Run the cell below to create our validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "val = train[:1000]\n",
    "train_final = train[1000:]\n",
    "label_val = label_train[:1000]\n",
    "label_train_final = label_train[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2000)\n",
      "(7500, 2000)\n",
      "(1000, 7)\n",
      "(7500, 7)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(val))\n",
    "print(np.shape(train_final))\n",
    "print(np.shape(label_val))\n",
    "print(np.shape(label_train_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2000)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Creating, compiling and running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import `Sequential` from the appropriate module in keras.\n",
    "* Import `Dense` from the appropriate module in keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build a model with the following specifications in the cell below:\n",
    "\n",
    "* An input layer of shape `(2000,)`\n",
    "* Hidden layer 1: Dense, 50 neurons, relu activation \n",
    "* Hidden layer 2: Dense, 25 neurons, relu activation\n",
    "* Output layer: Dense, 7 neurons, softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50,activation='relu', input_shape=(2000,))) \n",
    "model.add(Dense(25,activation='relu'))\n",
    "model.add(Dense(7,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, `compile` the model with the following settings:\n",
    "\n",
    "* Optimizer is `\"SGD\"`\n",
    "* Loss is `'categorical_crossentropy'`\n",
    "* metrics is `['accuracy']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Train the model for 120 epochs in mini-batches of 256 samples. Also pass in `(val, label_val)` to the `validation_data` parameter, so that we see how our model does on the test set after every epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 3s 398us/step - loss: 1.9346 - acc: 0.1589 - val_loss: 1.9237 - val_acc: 0.1520\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 2s 288us/step - loss: 1.9126 - acc: 0.1924 - val_loss: 1.9034 - val_acc: 0.1830\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 2s 286us/step - loss: 1.8925 - acc: 0.2225 - val_loss: 1.8833 - val_acc: 0.2250\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 2s 286us/step - loss: 1.8716 - acc: 0.2555 - val_loss: 1.8629 - val_acc: 0.2620\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 2s 286us/step - loss: 1.8480 - acc: 0.2783 - val_loss: 1.8391 - val_acc: 0.2920\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 2s 286us/step - loss: 1.8207 - acc: 0.3117 - val_loss: 1.8115 - val_acc: 0.3140\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 2s 290us/step - loss: 1.7895 - acc: 0.3369 - val_loss: 1.7802 - val_acc: 0.3390\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 2s 292us/step - loss: 1.7544 - acc: 0.3645 - val_loss: 1.7456 - val_acc: 0.3610\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 2s 312us/step - loss: 1.7155 - acc: 0.3956 - val_loss: 1.7063 - val_acc: 0.3840\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 2s 313us/step - loss: 1.6725 - acc: 0.4277 - val_loss: 1.6636 - val_acc: 0.4110\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 2s 310us/step - loss: 1.6253 - acc: 0.4585 - val_loss: 1.6161 - val_acc: 0.4370\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 2s 298us/step - loss: 1.5729 - acc: 0.4880 - val_loss: 1.5664 - val_acc: 0.4630\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 2s 292us/step - loss: 1.5180 - acc: 0.5164 - val_loss: 1.5140 - val_acc: 0.4940\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 2s 317us/step - loss: 1.4622 - acc: 0.5387 - val_loss: 1.4608 - val_acc: 0.5180\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 2s 318us/step - loss: 1.4066 - acc: 0.5641 - val_loss: 1.4081 - val_acc: 0.5360\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 2s 273us/step - loss: 1.3522 - acc: 0.5857 - val_loss: 1.3574 - val_acc: 0.5550\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 2s 267us/step - loss: 1.2998 - acc: 0.6083 - val_loss: 1.3086 - val_acc: 0.5740\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 2s 281us/step - loss: 1.2503 - acc: 0.6268 - val_loss: 1.2631 - val_acc: 0.5980\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 2s 270us/step - loss: 1.2035 - acc: 0.6412 - val_loss: 1.2222 - val_acc: 0.6120\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 2s 277us/step - loss: 1.1600 - acc: 0.6529 - val_loss: 1.1820 - val_acc: 0.6290\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 2s 270us/step - loss: 1.1193 - acc: 0.6652 - val_loss: 1.1461 - val_acc: 0.6290\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 2s 261us/step - loss: 1.0811 - acc: 0.6780 - val_loss: 1.1093 - val_acc: 0.6430\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 2s 250us/step - loss: 1.0455 - acc: 0.6853 - val_loss: 1.0819 - val_acc: 0.6420\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 2s 253us/step - loss: 1.0131 - acc: 0.6948 - val_loss: 1.0504 - val_acc: 0.6530\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 2s 238us/step - loss: 0.9818 - acc: 0.7027 - val_loss: 1.0221 - val_acc: 0.6540\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 2s 250us/step - loss: 0.9529 - acc: 0.7096 - val_loss: 0.9961 - val_acc: 0.6680\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 2s 328us/step - loss: 0.9257 - acc: 0.7187 - val_loss: 0.9695 - val_acc: 0.6690\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 2s 300us/step - loss: 0.9005 - acc: 0.7221 - val_loss: 0.9479 - val_acc: 0.6690\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 2s 313us/step - loss: 0.8767 - acc: 0.7288 - val_loss: 0.9283 - val_acc: 0.6810\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 2s 323us/step - loss: 0.8546 - acc: 0.7324 - val_loss: 0.9063 - val_acc: 0.6950\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 2s 323us/step - loss: 0.8344 - acc: 0.7387 - val_loss: 0.8885 - val_acc: 0.6890\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 3s 355us/step - loss: 0.8149 - acc: 0.7447 - val_loss: 0.8729 - val_acc: 0.6940\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 2s 294us/step - loss: 0.7972 - acc: 0.7471 - val_loss: 0.8570 - val_acc: 0.6970\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 2s 259us/step - loss: 0.7801 - acc: 0.7509 - val_loss: 0.8449 - val_acc: 0.6980\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 2s 267us/step - loss: 0.7641 - acc: 0.7573 - val_loss: 0.8321 - val_acc: 0.7080\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 2s 249us/step - loss: 0.7494 - acc: 0.7600 - val_loss: 0.8189 - val_acc: 0.7060\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 2s 259us/step - loss: 0.7355 - acc: 0.7645 - val_loss: 0.8075 - val_acc: 0.7140\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 2s 296us/step - loss: 0.7228 - acc: 0.7669 - val_loss: 0.7948 - val_acc: 0.7140\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 2s 271us/step - loss: 0.7105 - acc: 0.7716 - val_loss: 0.7862 - val_acc: 0.7140\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 2s 270us/step - loss: 0.6987 - acc: 0.7731 - val_loss: 0.7770 - val_acc: 0.7150\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 2s 297us/step - loss: 0.6877 - acc: 0.7775 - val_loss: 0.7683 - val_acc: 0.7230\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 2s 241us/step - loss: 0.6780 - acc: 0.7765 - val_loss: 0.7594 - val_acc: 0.7230\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 2s 240us/step - loss: 0.6676 - acc: 0.7808 - val_loss: 0.7549 - val_acc: 0.7210\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 2s 239us/step - loss: 0.6581 - acc: 0.7823 - val_loss: 0.7449 - val_acc: 0.7360\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 2s 239us/step - loss: 0.6499 - acc: 0.7851 - val_loss: 0.7394 - val_acc: 0.7240\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 2s 237us/step - loss: 0.6408 - acc: 0.7897 - val_loss: 0.7361 - val_acc: 0.7250\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 2s 244us/step - loss: 0.6325 - acc: 0.7897 - val_loss: 0.7287 - val_acc: 0.7230\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 2s 252us/step - loss: 0.6251 - acc: 0.7916 - val_loss: 0.7218 - val_acc: 0.7290\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 2s 244us/step - loss: 0.6173 - acc: 0.7945 - val_loss: 0.7221 - val_acc: 0.7310\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 2s 251us/step - loss: 0.6101 - acc: 0.7961 - val_loss: 0.7152 - val_acc: 0.7290\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 2s 240us/step - loss: 0.6030 - acc: 0.7983 - val_loss: 0.7125 - val_acc: 0.7330\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.5963 - acc: 0.7972 - val_loss: 0.7075 - val_acc: 0.7410\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.5903 - acc: 0.7991 - val_loss: 0.7065 - val_acc: 0.7370\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 2s 253us/step - loss: 0.5841 - acc: 0.8017 - val_loss: 0.6996 - val_acc: 0.7450\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 2s 245us/step - loss: 0.5780 - acc: 0.8040 - val_loss: 0.6939 - val_acc: 0.7470\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 2s 238us/step - loss: 0.5721 - acc: 0.8063 - val_loss: 0.6923 - val_acc: 0.7440\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 2s 239us/step - loss: 0.5667 - acc: 0.8071 - val_loss: 0.6885 - val_acc: 0.7460\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 2s 238us/step - loss: 0.5610 - acc: 0.8115 - val_loss: 0.6851 - val_acc: 0.7480\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 2s 257us/step - loss: 0.5555 - acc: 0.8124 - val_loss: 0.6826 - val_acc: 0.7500\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 2s 244us/step - loss: 0.5501 - acc: 0.8135 - val_loss: 0.6830 - val_acc: 0.7480\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 2s 239us/step - loss: 0.5451 - acc: 0.8155 - val_loss: 0.6761 - val_acc: 0.7500\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 2s 236us/step - loss: 0.5405 - acc: 0.8168 - val_loss: 0.6780 - val_acc: 0.7510\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 2s 238us/step - loss: 0.5354 - acc: 0.8175 - val_loss: 0.6848 - val_acc: 0.7480\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 2s 238us/step - loss: 0.5309 - acc: 0.8201 - val_loss: 0.6707 - val_acc: 0.7550\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 2s 238us/step - loss: 0.5261 - acc: 0.8211 - val_loss: 0.6698 - val_acc: 0.7540\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 2s 237us/step - loss: 0.5217 - acc: 0.8244 - val_loss: 0.6710 - val_acc: 0.7520\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 2s 237us/step - loss: 0.5170 - acc: 0.8228 - val_loss: 0.6705 - val_acc: 0.7500\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 2s 239us/step - loss: 0.5127 - acc: 0.8252 - val_loss: 0.6718 - val_acc: 0.7520\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 2s 236us/step - loss: 0.5086 - acc: 0.8280 - val_loss: 0.6671 - val_acc: 0.7570\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 2s 236us/step - loss: 0.5041 - acc: 0.8272 - val_loss: 0.6594 - val_acc: 0.7590\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 2s 238us/step - loss: 0.5003 - acc: 0.8292 - val_loss: 0.6580 - val_acc: 0.7620\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 2s 238us/step - loss: 0.4961 - acc: 0.8303 - val_loss: 0.6609 - val_acc: 0.7590\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 2s 238us/step - loss: 0.4924 - acc: 0.8324 - val_loss: 0.6585 - val_acc: 0.7600\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.4880 - acc: 0.8340 - val_loss: 0.6585 - val_acc: 0.7570\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 2s 257us/step - loss: 0.4837 - acc: 0.8339 - val_loss: 0.6616 - val_acc: 0.7600\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.4799 - acc: 0.8344 - val_loss: 0.6534 - val_acc: 0.7580\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 2s 243us/step - loss: 0.4766 - acc: 0.8376 - val_loss: 0.6522 - val_acc: 0.7610\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 2s 260us/step - loss: 0.4730 - acc: 0.8397 - val_loss: 0.6561 - val_acc: 0.7600\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 2s 296us/step - loss: 0.4690 - acc: 0.8415 - val_loss: 0.6653 - val_acc: 0.7530\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.4658 - acc: 0.8420 - val_loss: 0.6556 - val_acc: 0.7580\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 2s 249us/step - loss: 0.4620 - acc: 0.8421 - val_loss: 0.6544 - val_acc: 0.7610\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 2s 271us/step - loss: 0.4583 - acc: 0.8433 - val_loss: 0.6505 - val_acc: 0.7620\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 2s 288us/step - loss: 0.4548 - acc: 0.8455 - val_loss: 0.6476 - val_acc: 0.7620\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 2s 260us/step - loss: 0.4516 - acc: 0.8452 - val_loss: 0.6503 - val_acc: 0.7610\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.4480 - acc: 0.8472 - val_loss: 0.6502 - val_acc: 0.7630\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 2s 244us/step - loss: 0.4446 - acc: 0.8501 - val_loss: 0.6532 - val_acc: 0.7600\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 2s 253us/step - loss: 0.4414 - acc: 0.8511 - val_loss: 0.6520 - val_acc: 0.7600\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 2s 244us/step - loss: 0.4381 - acc: 0.8532 - val_loss: 0.6479 - val_acc: 0.7610\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 2s 238us/step - loss: 0.4350 - acc: 0.8533 - val_loss: 0.6474 - val_acc: 0.7580\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 2s 239us/step - loss: 0.4316 - acc: 0.8516 - val_loss: 0.6440 - val_acc: 0.7610\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 2s 240us/step - loss: 0.4281 - acc: 0.8555 - val_loss: 0.6497 - val_acc: 0.7620\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 2s 237us/step - loss: 0.4254 - acc: 0.8551 - val_loss: 0.6439 - val_acc: 0.7620\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 2s 243us/step - loss: 0.4221 - acc: 0.8569 - val_loss: 0.6446 - val_acc: 0.7590\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 2s 253us/step - loss: 0.4193 - acc: 0.8579 - val_loss: 0.6491 - val_acc: 0.7580\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 2s 258us/step - loss: 0.4160 - acc: 0.8585 - val_loss: 0.6456 - val_acc: 0.7590\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 2s 251us/step - loss: 0.4127 - acc: 0.8601 - val_loss: 0.6464 - val_acc: 0.7580\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 2s 245us/step - loss: 0.4101 - acc: 0.8612 - val_loss: 0.6420 - val_acc: 0.7630\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 2s 242us/step - loss: 0.4073 - acc: 0.8628 - val_loss: 0.6457 - val_acc: 0.7610\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 2s 242us/step - loss: 0.4043 - acc: 0.8636 - val_loss: 0.6458 - val_acc: 0.7590\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 2s 253us/step - loss: 0.4012 - acc: 0.8663 - val_loss: 0.6516 - val_acc: 0.7550\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 2s 307us/step - loss: 0.3986 - acc: 0.8665 - val_loss: 0.6449 - val_acc: 0.7600\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 2s 249us/step - loss: 0.3953 - acc: 0.8675 - val_loss: 0.6513 - val_acc: 0.7620\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 2s 241us/step - loss: 0.3924 - acc: 0.8681 - val_loss: 0.6532 - val_acc: 0.7630\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 2s 322us/step - loss: 0.3896 - acc: 0.8703 - val_loss: 0.6495 - val_acc: 0.7620\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 3s 365us/step - loss: 0.3872 - acc: 0.8692 - val_loss: 0.6409 - val_acc: 0.7660\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 3s 349us/step - loss: 0.3842 - acc: 0.8723 - val_loss: 0.6450 - val_acc: 0.7650\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 2s 320us/step - loss: 0.3812 - acc: 0.8733 - val_loss: 0.6574 - val_acc: 0.7580\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 2s 322us/step - loss: 0.3791 - acc: 0.8733 - val_loss: 0.6460 - val_acc: 0.7620\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 2s 321us/step - loss: 0.3760 - acc: 0.8735 - val_loss: 0.6528 - val_acc: 0.7650\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 2s 313us/step - loss: 0.3735 - acc: 0.8764 - val_loss: 0.6421 - val_acc: 0.7660\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 2s 315us/step - loss: 0.3712 - acc: 0.8775 - val_loss: 0.6521 - val_acc: 0.7570\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 2s 317us/step - loss: 0.3689 - acc: 0.8784 - val_loss: 0.6469 - val_acc: 0.7610\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 2s 323us/step - loss: 0.3665 - acc: 0.8779 - val_loss: 0.6556 - val_acc: 0.7620\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 2s 300us/step - loss: 0.3634 - acc: 0.8797 - val_loss: 0.6458 - val_acc: 0.7650\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 2s 302us/step - loss: 0.3610 - acc: 0.8801 - val_loss: 0.6573 - val_acc: 0.7600\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 2s 329us/step - loss: 0.3582 - acc: 0.8816 - val_loss: 0.6560 - val_acc: 0.7590\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 2s 320us/step - loss: 0.3561 - acc: 0.8827 - val_loss: 0.6549 - val_acc: 0.7610\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 2s 317us/step - loss: 0.3537 - acc: 0.8833 - val_loss: 0.6546 - val_acc: 0.7560\n",
      "Epoch 119/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 2s 331us/step - loss: 0.3508 - acc: 0.8845 - val_loss: 0.6511 - val_acc: 0.7560\n",
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 2s 314us/step - loss: 0.3489 - acc: 0.8837 - val_loss: 0.6498 - val_acc: 0.7600\n"
     ]
    }
   ],
   "source": [
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Store the model's `.history` inside of `model_val_dict`\n",
    "* Check what `keys()` this dictionary contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the final results on the training and testing sets using `model.evaluate()` on `train_final` and `label_train_final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 2s 262us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also use this function to get the results on our testing set.  Call the function again, but this time on `test` and `label_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 303us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check the contents of each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3448504872441292, 0.8893333333333333]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Results: [0.33576024494171142, 0.89600000000000002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6270274307727813, 0.7593333333333333]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Results: [0.72006658554077152, 0.74333333365122478]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [1.923747929573059,\n",
       "  1.903404146194458,\n",
       "  1.8833405055999757,\n",
       "  1.8629248723983765,\n",
       "  1.8390928010940553,\n",
       "  1.811545934677124,\n",
       "  1.7802221660614013,\n",
       "  1.7456075506210327,\n",
       "  1.7062777719497682,\n",
       "  1.6636336421966553,\n",
       "  1.6161155662536622,\n",
       "  1.5664185409545899,\n",
       "  1.514007466316223,\n",
       "  1.4607902345657349,\n",
       "  1.4080764598846436,\n",
       "  1.3574026412963867,\n",
       "  1.308613842010498,\n",
       "  1.2630957221984864,\n",
       "  1.222189609527588,\n",
       "  1.1819880418777466,\n",
       "  1.1461393404006959,\n",
       "  1.109346872329712,\n",
       "  1.081881417274475,\n",
       "  1.0503777275085449,\n",
       "  1.0221461381912231,\n",
       "  0.99610675907135,\n",
       "  0.9694620456695556,\n",
       "  0.9478779697418213,\n",
       "  0.9282625575065613,\n",
       "  0.9063040251731873,\n",
       "  0.8884898209571839,\n",
       "  0.8728558807373047,\n",
       "  0.8569863328933716,\n",
       "  0.8448549547195434,\n",
       "  0.8320631852149963,\n",
       "  0.8188950366973877,\n",
       "  0.8075011076927185,\n",
       "  0.7947821936607361,\n",
       "  0.7862267265319824,\n",
       "  0.7770417056083679,\n",
       "  0.7682788643836975,\n",
       "  0.7594435095787049,\n",
       "  0.7548910655975342,\n",
       "  0.744863525390625,\n",
       "  0.7393947443962097,\n",
       "  0.736120397567749,\n",
       "  0.7286795902252198,\n",
       "  0.7217977356910705,\n",
       "  0.7220618262290954,\n",
       "  0.7152116737365722,\n",
       "  0.7124935517311096,\n",
       "  0.7074935517311096,\n",
       "  0.7065257201194763,\n",
       "  0.6995649542808533,\n",
       "  0.6939284720420837,\n",
       "  0.6922704434394836,\n",
       "  0.6885286836624146,\n",
       "  0.6850940003395081,\n",
       "  0.6826108222007752,\n",
       "  0.6830485100746155,\n",
       "  0.6761129312515258,\n",
       "  0.6779535322189331,\n",
       "  0.6848145475387574,\n",
       "  0.6707369823455811,\n",
       "  0.6697601175308228,\n",
       "  0.6709523682594299,\n",
       "  0.6705172719955445,\n",
       "  0.6717969045639038,\n",
       "  0.6671467423439026,\n",
       "  0.6593734784126282,\n",
       "  0.6580452837944031,\n",
       "  0.6609021692276001,\n",
       "  0.6584827218055725,\n",
       "  0.6585046257972718,\n",
       "  0.6616275768280029,\n",
       "  0.6534010362625122,\n",
       "  0.6521647658348083,\n",
       "  0.6561027555465698,\n",
       "  0.6652758326530457,\n",
       "  0.6556121726036072,\n",
       "  0.654445405960083,\n",
       "  0.650466480255127,\n",
       "  0.6475911169052124,\n",
       "  0.6502932558059692,\n",
       "  0.650183590888977,\n",
       "  0.653164439201355,\n",
       "  0.6520344038009643,\n",
       "  0.6478539800643921,\n",
       "  0.6473978371620178,\n",
       "  0.6440464062690735,\n",
       "  0.649723937034607,\n",
       "  0.6439150304794311,\n",
       "  0.6446086468696595,\n",
       "  0.6491310772895813,\n",
       "  0.6455914297103882,\n",
       "  0.6464154634475708,\n",
       "  0.6419502377510071,\n",
       "  0.6456741118431091,\n",
       "  0.6458227190971374,\n",
       "  0.6515796051025391,\n",
       "  0.6448764390945435,\n",
       "  0.6513044576644897,\n",
       "  0.6532016606330872,\n",
       "  0.6494807033538819,\n",
       "  0.6409436640739441,\n",
       "  0.6449817981719971,\n",
       "  0.6573516492843627,\n",
       "  0.6459651522636414,\n",
       "  0.6527724118232727,\n",
       "  0.6420530104637145,\n",
       "  0.6521277074813843,\n",
       "  0.646873462677002,\n",
       "  0.6556330075263977,\n",
       "  0.6458055200576782,\n",
       "  0.6573349294662476,\n",
       "  0.6560355491638183,\n",
       "  0.6549284029006958,\n",
       "  0.6545982193946839,\n",
       "  0.651104419708252,\n",
       "  0.6497881178855895],\n",
       " 'val_acc': [0.152,\n",
       "  0.18300000166893005,\n",
       "  0.22500000154972077,\n",
       "  0.26200000011920926,\n",
       "  0.2919999973773956,\n",
       "  0.31400000071525574,\n",
       "  0.3389999992847443,\n",
       "  0.3609999978542328,\n",
       "  0.3840000011920929,\n",
       "  0.41099999856948854,\n",
       "  0.43699999833106995,\n",
       "  0.4629999992847443,\n",
       "  0.4939999990463257,\n",
       "  0.5180000023841858,\n",
       "  0.5360000023841858,\n",
       "  0.555,\n",
       "  0.5739999985694886,\n",
       "  0.5980000052452087,\n",
       "  0.6119999947547913,\n",
       "  0.6290000004768371,\n",
       "  0.6290000061988831,\n",
       "  0.6430000014305115,\n",
       "  0.6419999980926514,\n",
       "  0.6529999966621399,\n",
       "  0.6540000023841858,\n",
       "  0.6679999942779541,\n",
       "  0.6689999942779541,\n",
       "  0.6690000023841858,\n",
       "  0.6810000033378601,\n",
       "  0.695,\n",
       "  0.6890000057220459,\n",
       "  0.6940000057220459,\n",
       "  0.6969999976158142,\n",
       "  0.6980000009536743,\n",
       "  0.7080000009536743,\n",
       "  0.7060000066757202,\n",
       "  0.7140000057220459,\n",
       "  0.7140000033378601,\n",
       "  0.7139999952316284,\n",
       "  0.7150000009536743,\n",
       "  0.7230000042915344,\n",
       "  0.7230000009536743,\n",
       "  0.7210000066757202,\n",
       "  0.7359999938011169,\n",
       "  0.7240000033378601,\n",
       "  0.7250000066757202,\n",
       "  0.7230000033378601,\n",
       "  0.7290000066757202,\n",
       "  0.7310000033378601,\n",
       "  0.7289999952316284,\n",
       "  0.7330000009536743,\n",
       "  0.7409999961853028,\n",
       "  0.7370000009536744,\n",
       "  0.7450000042915345,\n",
       "  0.7470000019073486,\n",
       "  0.7439999985694885,\n",
       "  0.7459999961853028,\n",
       "  0.7479999961853028,\n",
       "  0.7500000019073486,\n",
       "  0.7480000019073486,\n",
       "  0.7500000052452087,\n",
       "  0.7509999995231629,\n",
       "  0.7479999938011169,\n",
       "  0.7549999995231629,\n",
       "  0.7539999995231629,\n",
       "  0.7519999938011169,\n",
       "  0.7500000052452087,\n",
       "  0.7519999938011169,\n",
       "  0.756999997138977,\n",
       "  0.759000002861023,\n",
       "  0.7619999947547913,\n",
       "  0.7589999995231629,\n",
       "  0.7599999947547913,\n",
       "  0.7570000052452087,\n",
       "  0.760000002861023,\n",
       "  0.7580000061988831,\n",
       "  0.761000002861023,\n",
       "  0.7600000052452087,\n",
       "  0.7530000052452087,\n",
       "  0.7579999938011169,\n",
       "  0.761000002861023,\n",
       "  0.7620000004768371,\n",
       "  0.7620000061988831,\n",
       "  0.7610000004768371,\n",
       "  0.7630000004768371,\n",
       "  0.759999997138977,\n",
       "  0.760000002861023,\n",
       "  0.7610000004768371,\n",
       "  0.7580000061988831,\n",
       "  0.7610000061988831,\n",
       "  0.761999997138977,\n",
       "  0.7619999947547913,\n",
       "  0.7590000004768371,\n",
       "  0.7579999947547913,\n",
       "  0.7589999947547913,\n",
       "  0.7579999947547913,\n",
       "  0.7630000061988831,\n",
       "  0.7610000004768371,\n",
       "  0.7589999947547913,\n",
       "  0.754999997138977,\n",
       "  0.7600000004768371,\n",
       "  0.7619999947547913,\n",
       "  0.763000002861023,\n",
       "  0.7620000004768371,\n",
       "  0.7659999980926514,\n",
       "  0.7649999980926514,\n",
       "  0.758000002861023,\n",
       "  0.7620000061988831,\n",
       "  0.7650000061988831,\n",
       "  0.7659999957084656,\n",
       "  0.756999997138977,\n",
       "  0.7609999947547913,\n",
       "  0.762000002861023,\n",
       "  0.7650000061988831,\n",
       "  0.759999997138977,\n",
       "  0.759000002861023,\n",
       "  0.7610000004768371,\n",
       "  0.755999997138977,\n",
       "  0.756000002861023,\n",
       "  0.7599999947547913],\n",
       " 'loss': [1.9345825971603394,\n",
       "  1.9126346522013347,\n",
       "  1.8925258342742919,\n",
       "  1.8715945004781087,\n",
       "  1.8480412066777547,\n",
       "  1.82066009572347,\n",
       "  1.7895395788828532,\n",
       "  1.7544100570042929,\n",
       "  1.7155240778605143,\n",
       "  1.6724765634536742,\n",
       "  1.6252793525695801,\n",
       "  1.5729112608591715,\n",
       "  1.5179902954737345,\n",
       "  1.4621810963312785,\n",
       "  1.406608085568746,\n",
       "  1.3521544860839845,\n",
       "  1.2997510793685914,\n",
       "  1.2503289201100667,\n",
       "  1.203494769414266,\n",
       "  1.160048930422465,\n",
       "  1.1193496803919474,\n",
       "  1.0810870038350424,\n",
       "  1.045512826983134,\n",
       "  1.013079752031962,\n",
       "  0.981783533445994,\n",
       "  0.9529436820983886,\n",
       "  0.9257074350992839,\n",
       "  0.9004893362045288,\n",
       "  0.8767274942715962,\n",
       "  0.8545652198473612,\n",
       "  0.8343935068766276,\n",
       "  0.814914963499705,\n",
       "  0.7972084095637003,\n",
       "  0.7800678763389587,\n",
       "  0.76413684425354,\n",
       "  0.7494120573361714,\n",
       "  0.7355165252685547,\n",
       "  0.7228377420743306,\n",
       "  0.7104758774439494,\n",
       "  0.6986864017804464,\n",
       "  0.6877426735242208,\n",
       "  0.6779530068397522,\n",
       "  0.6676067282358805,\n",
       "  0.658065604464213,\n",
       "  0.6499130667368571,\n",
       "  0.6407728743553162,\n",
       "  0.6325235808054606,\n",
       "  0.6250616772651673,\n",
       "  0.6173409515062968,\n",
       "  0.6101180401484172,\n",
       "  0.6030058870633443,\n",
       "  0.5962631187438965,\n",
       "  0.5903051260312399,\n",
       "  0.5841172127405803,\n",
       "  0.577994980875651,\n",
       "  0.5720908408800761,\n",
       "  0.5666609981854757,\n",
       "  0.5610049946626028,\n",
       "  0.5555044935862223,\n",
       "  0.5501139426549275,\n",
       "  0.5450864238103231,\n",
       "  0.5404997049967448,\n",
       "  0.5354106951395671,\n",
       "  0.5308830762545268,\n",
       "  0.5260862547874451,\n",
       "  0.5217356549580892,\n",
       "  0.5169927997430166,\n",
       "  0.5126821272532145,\n",
       "  0.5085640512784322,\n",
       "  0.5040596530596415,\n",
       "  0.5002956854820252,\n",
       "  0.49611249113082884,\n",
       "  0.49241532405217486,\n",
       "  0.48795078546206155,\n",
       "  0.4836579488913218,\n",
       "  0.47989369746843974,\n",
       "  0.4765501053969065,\n",
       "  0.47302182610829674,\n",
       "  0.4689931272506714,\n",
       "  0.46582225319544474,\n",
       "  0.4619589567979177,\n",
       "  0.45825607719421385,\n",
       "  0.45479995352427166,\n",
       "  0.45157975268363953,\n",
       "  0.4480344431241353,\n",
       "  0.44459411730766296,\n",
       "  0.4413734585762024,\n",
       "  0.4380570805549622,\n",
       "  0.4349685626029968,\n",
       "  0.4315853018601735,\n",
       "  0.428092853196462,\n",
       "  0.42542319092750547,\n",
       "  0.42211044114430746,\n",
       "  0.41932852210998534,\n",
       "  0.41598969623247783,\n",
       "  0.4126885234832764,\n",
       "  0.41007244176864627,\n",
       "  0.40729398840268455,\n",
       "  0.40426548198064166,\n",
       "  0.401245018196106,\n",
       "  0.3985893280029297,\n",
       "  0.3952556461016337,\n",
       "  0.3924118968645732,\n",
       "  0.38964378620783485,\n",
       "  0.38724508883158365,\n",
       "  0.3841904411633809,\n",
       "  0.38122653505007426,\n",
       "  0.3790641810099284,\n",
       "  0.37600513881047565,\n",
       "  0.37347389935652414,\n",
       "  0.3712118348439534,\n",
       "  0.3689263990402222,\n",
       "  0.36646100749969485,\n",
       "  0.3633586891333262,\n",
       "  0.3610146311124166,\n",
       "  0.35823562904993694,\n",
       "  0.3561321019808451,\n",
       "  0.3536617096265157,\n",
       "  0.3508084650357564,\n",
       "  0.3488645915826162],\n",
       " 'acc': [0.15893333338101706,\n",
       "  0.19239999996026358,\n",
       "  0.2225333333492279,\n",
       "  0.25546666673024493,\n",
       "  0.2782666665395101,\n",
       "  0.3117333332856496,\n",
       "  0.33693333326975505,\n",
       "  0.36453333338101707,\n",
       "  0.39559999990463257,\n",
       "  0.42773333338101704,\n",
       "  0.4585333332061768,\n",
       "  0.4880000002861023,\n",
       "  0.5163999999205271,\n",
       "  0.538666666952769,\n",
       "  0.564133333492279,\n",
       "  0.5857333330790202,\n",
       "  0.6082666666030884,\n",
       "  0.6268000001589458,\n",
       "  0.6412000000635782,\n",
       "  0.6529333335240682,\n",
       "  0.6652000001271565,\n",
       "  0.6780000001907348,\n",
       "  0.6853333332061767,\n",
       "  0.6947999998410542,\n",
       "  0.7026666667938233,\n",
       "  0.7096000000953674,\n",
       "  0.7186666666666667,\n",
       "  0.7221333334287008,\n",
       "  0.7288,\n",
       "  0.7324000000317892,\n",
       "  0.7386666668891907,\n",
       "  0.7446666664759318,\n",
       "  0.7470666669527689,\n",
       "  0.7509333332061767,\n",
       "  0.7573333331108093,\n",
       "  0.7600000001907349,\n",
       "  0.7645333336194357,\n",
       "  0.7669333332379659,\n",
       "  0.7716,\n",
       "  0.7730666666984558,\n",
       "  0.7774666664441426,\n",
       "  0.7765333332379659,\n",
       "  0.7808000001589457,\n",
       "  0.7822666669527689,\n",
       "  0.7850666664759318,\n",
       "  0.789733333492279,\n",
       "  0.7897333335240682,\n",
       "  0.791599999777476,\n",
       "  0.7945333333969116,\n",
       "  0.7961333333015442,\n",
       "  0.7982666665395101,\n",
       "  0.7971999999682109,\n",
       "  0.7990666665712992,\n",
       "  0.8017333332061768,\n",
       "  0.8039999998728434,\n",
       "  0.8062666666348776,\n",
       "  0.8070666668891907,\n",
       "  0.8114666667302449,\n",
       "  0.8123999997456869,\n",
       "  0.8134666669527689,\n",
       "  0.8154666664441427,\n",
       "  0.8167999997456868,\n",
       "  0.8174666668574015,\n",
       "  0.8201333332061768,\n",
       "  0.8210666664123535,\n",
       "  0.8243999998728434,\n",
       "  0.8227999999682108,\n",
       "  0.8252000002543132,\n",
       "  0.8280000001589457,\n",
       "  0.8271999998728434,\n",
       "  0.8292000000953674,\n",
       "  0.8302666666666667,\n",
       "  0.8324000002543132,\n",
       "  0.8340000001589457,\n",
       "  0.8338666669209798,\n",
       "  0.8343999997456869,\n",
       "  0.8375999998410543,\n",
       "  0.8397333335240682,\n",
       "  0.8414666668574016,\n",
       "  0.8419999999682108,\n",
       "  0.8421333335876465,\n",
       "  0.843333333269755,\n",
       "  0.8454666666666667,\n",
       "  0.8452000001589457,\n",
       "  0.8472000001589457,\n",
       "  0.8501333332061768,\n",
       "  0.8510666665077209,\n",
       "  0.8531999999364217,\n",
       "  0.8533333333969116,\n",
       "  0.8516000000635783,\n",
       "  0.8554666666348775,\n",
       "  0.8550666665077209,\n",
       "  0.8569333335876465,\n",
       "  0.8578666668574015,\n",
       "  0.8585333330790201,\n",
       "  0.8601333330790202,\n",
       "  0.8612000001589457,\n",
       "  0.8627999997456869,\n",
       "  0.8635999999364217,\n",
       "  0.866266666952769,\n",
       "  0.8665333333015441,\n",
       "  0.8674666664441426,\n",
       "  0.8681333330790202,\n",
       "  0.870266666730245,\n",
       "  0.8691999998410542,\n",
       "  0.8722666664441426,\n",
       "  0.873333333269755,\n",
       "  0.8733333333651224,\n",
       "  0.8734666666030884,\n",
       "  0.876400000222524,\n",
       "  0.8774666666984559,\n",
       "  0.8783999998410543,\n",
       "  0.8778666667938232,\n",
       "  0.8797333331743876,\n",
       "  0.8801333330790202,\n",
       "  0.8816000000317892,\n",
       "  0.8826666666984558,\n",
       "  0.8833333333969117,\n",
       "  0.884533333492279,\n",
       "  0.883733333269755]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results. Let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy.\n",
    "\n",
    "Run the cell below to visualize a plot of our training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'g.', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! \n",
    "\n",
    "Run the cell below to visualize a plot of our training and validation accuracy>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'r.', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Early stopping\n",
    "\n",
    "Now that we know that the model starts to overfit around epoch 60, we can just retrain the model from scratch, but this time only up to 60 epochs! This will help us with our overfitting problem.  This method is called **_Early Stopping_**.\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the exact model we did above. \n",
    "* Compile the model with the exact same hyperparameters.\n",
    "* Fit the model with the exact same hyperparameters, with the exception of `epochs`.  This time, set epochs to `60` instead of `120`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 3s 339us/step - loss: 1.9573 - acc: 0.1564 - val_loss: 1.9420 - val_acc: 0.1790\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 2s 311us/step - loss: 1.9289 - acc: 0.1921 - val_loss: 1.9207 - val_acc: 0.2010\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 2s 292us/step - loss: 1.9077 - acc: 0.2175 - val_loss: 1.9015 - val_acc: 0.2220\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 2s 302us/step - loss: 1.8862 - acc: 0.2333 - val_loss: 1.8793 - val_acc: 0.2350\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 2s 311us/step - loss: 1.8615 - acc: 0.2452 - val_loss: 1.8537 - val_acc: 0.2630\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 2s 331us/step - loss: 1.8318 - acc: 0.2715 - val_loss: 1.8214 - val_acc: 0.2770\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 2s 331us/step - loss: 1.7967 - acc: 0.2912 - val_loss: 1.7853 - val_acc: 0.3040\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 2s 301us/step - loss: 1.7576 - acc: 0.3240 - val_loss: 1.7446 - val_acc: 0.3180\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 2s 305us/step - loss: 1.7145 - acc: 0.3528 - val_loss: 1.7007 - val_acc: 0.3530\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 2s 292us/step - loss: 1.6683 - acc: 0.3867 - val_loss: 1.6538 - val_acc: 0.3820\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 2s 292us/step - loss: 1.6192 - acc: 0.4288 - val_loss: 1.6050 - val_acc: 0.4230\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 2s 293us/step - loss: 1.5684 - acc: 0.4656 - val_loss: 1.5559 - val_acc: 0.4720\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 2s 291us/step - loss: 1.5167 - acc: 0.4981 - val_loss: 1.5055 - val_acc: 0.4910\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 2s 291us/step - loss: 1.4647 - acc: 0.5213 - val_loss: 1.4555 - val_acc: 0.5320\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 2s 293us/step - loss: 1.4124 - acc: 0.5473 - val_loss: 1.4068 - val_acc: 0.5450\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 2s 294us/step - loss: 1.3610 - acc: 0.5668 - val_loss: 1.3582 - val_acc: 0.5580\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 2s 305us/step - loss: 1.3109 - acc: 0.5833 - val_loss: 1.3129 - val_acc: 0.5720\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 2s 317us/step - loss: 1.2625 - acc: 0.6035 - val_loss: 1.2698 - val_acc: 0.5870\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 2s 301us/step - loss: 1.2169 - acc: 0.6169 - val_loss: 1.2276 - val_acc: 0.5990\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 2s 300us/step - loss: 1.1735 - acc: 0.6283 - val_loss: 1.1909 - val_acc: 0.6130\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 2s 295us/step - loss: 1.1334 - acc: 0.6409 - val_loss: 1.1532 - val_acc: 0.6230\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 2s 296us/step - loss: 1.0954 - acc: 0.6497 - val_loss: 1.1202 - val_acc: 0.6300\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 2s 297us/step - loss: 1.0601 - acc: 0.6583 - val_loss: 1.0919 - val_acc: 0.6410\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 2s 324us/step - loss: 1.0277 - acc: 0.6663 - val_loss: 1.0616 - val_acc: 0.6440\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 2s 307us/step - loss: 0.9972 - acc: 0.6777 - val_loss: 1.0383 - val_acc: 0.6560\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 2s 298us/step - loss: 0.9690 - acc: 0.6892 - val_loss: 1.0101 - val_acc: 0.6610\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 2s 297us/step - loss: 0.9431 - acc: 0.6917 - val_loss: 0.9894 - val_acc: 0.6640\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 2s 295us/step - loss: 0.9192 - acc: 0.7005 - val_loss: 0.9685 - val_acc: 0.6760\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 2s 295us/step - loss: 0.8969 - acc: 0.7079 - val_loss: 0.9504 - val_acc: 0.6710\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 2s 297us/step - loss: 0.8760 - acc: 0.7116 - val_loss: 0.9323 - val_acc: 0.6810\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 2s 297us/step - loss: 0.8571 - acc: 0.7193 - val_loss: 0.9189 - val_acc: 0.6740\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 2s 305us/step - loss: 0.8392 - acc: 0.7215 - val_loss: 0.9014 - val_acc: 0.6850\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 2s 300us/step - loss: 0.8221 - acc: 0.7269 - val_loss: 0.8872 - val_acc: 0.6890\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 2s 305us/step - loss: 0.8069 - acc: 0.7323 - val_loss: 0.8809 - val_acc: 0.6910\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 2s 311us/step - loss: 0.7919 - acc: 0.7335 - val_loss: 0.8646 - val_acc: 0.6930\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 2s 302us/step - loss: 0.7780 - acc: 0.7411 - val_loss: 0.8508 - val_acc: 0.6960\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 2s 309us/step - loss: 0.7649 - acc: 0.7423 - val_loss: 0.8396 - val_acc: 0.6980\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 2s 294us/step - loss: 0.7523 - acc: 0.7491 - val_loss: 0.8351 - val_acc: 0.7020\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 2s 307us/step - loss: 0.7407 - acc: 0.7520 - val_loss: 0.8261 - val_acc: 0.6990\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 2s 328us/step - loss: 0.7294 - acc: 0.7533 - val_loss: 0.8156 - val_acc: 0.7050\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 2s 333us/step - loss: 0.7184 - acc: 0.7572 - val_loss: 0.8060 - val_acc: 0.7080\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 2s 329us/step - loss: 0.7081 - acc: 0.7611 - val_loss: 0.8002 - val_acc: 0.7130\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 2s 308us/step - loss: 0.6986 - acc: 0.7648 - val_loss: 0.7927 - val_acc: 0.7120\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 2s 312us/step - loss: 0.6893 - acc: 0.7703 - val_loss: 0.7846 - val_acc: 0.7150\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 2s 303us/step - loss: 0.6802 - acc: 0.7705 - val_loss: 0.7780 - val_acc: 0.7190\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 2s 312us/step - loss: 0.6720 - acc: 0.7719 - val_loss: 0.7734 - val_acc: 0.7220\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 2s 314us/step - loss: 0.6635 - acc: 0.7763 - val_loss: 0.7671 - val_acc: 0.7210\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 2s 301us/step - loss: 0.6556 - acc: 0.7776 - val_loss: 0.7605 - val_acc: 0.7230\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 2s 323us/step - loss: 0.6478 - acc: 0.7815 - val_loss: 0.7590 - val_acc: 0.7270\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 2s 316us/step - loss: 0.6402 - acc: 0.7836 - val_loss: 0.7523 - val_acc: 0.7290\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 2s 314us/step - loss: 0.6330 - acc: 0.7855 - val_loss: 0.7472 - val_acc: 0.7270\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 2s 303us/step - loss: 0.6260 - acc: 0.7893 - val_loss: 0.7407 - val_acc: 0.7290\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 2s 304us/step - loss: 0.6190 - acc: 0.7880 - val_loss: 0.7388 - val_acc: 0.7340\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 2s 303us/step - loss: 0.6127 - acc: 0.7909 - val_loss: 0.7343 - val_acc: 0.7360\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 2s 302us/step - loss: 0.6062 - acc: 0.7939 - val_loss: 0.7290 - val_acc: 0.7370\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 2s 299us/step - loss: 0.5998 - acc: 0.7975 - val_loss: 0.7283 - val_acc: 0.7350\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 2s 301us/step - loss: 0.5941 - acc: 0.7976 - val_loss: 0.7222 - val_acc: 0.7410\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 2s 306us/step - loss: 0.5880 - acc: 0.8023 - val_loss: 0.7182 - val_acc: 0.7430\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 2s 321us/step - loss: 0.5825 - acc: 0.8017 - val_loss: 0.7152 - val_acc: 0.7390\n",
      "Epoch 60/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 3s 344us/step - loss: 0.5764 - acc: 0.8060 - val_loss: 0.7130 - val_acc: 0.7430\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we did before, get our results using `model.evaluate()` on the appropriate variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 2s 264us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final,label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 292us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5715655176957448, 0.8081333333651225]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train  # Expected Output: [0.58606486314137773, 0.79826666669845581]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6986736919085185, 0.7393333333333333]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # [0.74768974288304646, 0.71333333365122475]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument `kernel_regulizers.l2` and adding a value for the regularization parameter lambda between parentheses.\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the same model we did before.\n",
    "* In our two hidden layers (but not our output layer), add in the parameter `kernel_regularizer=regularizers.l2(0.005)` to add L2 regularization to each hidden layer.  \n",
    "* Compile the model with the same hyperparameters as we did before. \n",
    "* Fit the model with the same hyperparameters as we did before, but this time for `120` epochs.\n",
    "* Store the fitted model that the `.fit` call returns inside a variable called `L2_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 3s 376us/step - loss: 2.6094 - acc: 0.1335 - val_loss: 2.5942 - val_acc: 0.1510\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 2s 293us/step - loss: 2.5797 - acc: 0.1519 - val_loss: 2.5717 - val_acc: 0.1670\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 2s 294us/step - loss: 2.5586 - acc: 0.1655 - val_loss: 2.5523 - val_acc: 0.1680\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 2s 292us/step - loss: 2.5387 - acc: 0.1849 - val_loss: 2.5333 - val_acc: 0.1880\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 2s 294us/step - loss: 2.5178 - acc: 0.2052 - val_loss: 2.5130 - val_acc: 0.2190\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 2s 293us/step - loss: 2.4952 - acc: 0.2385 - val_loss: 2.4902 - val_acc: 0.2480\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 2s 293us/step - loss: 2.4703 - acc: 0.2673 - val_loss: 2.4646 - val_acc: 0.2650\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 2s 294us/step - loss: 2.4424 - acc: 0.2908 - val_loss: 2.4360 - val_acc: 0.2890\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 2s 332us/step - loss: 2.4109 - acc: 0.3177 - val_loss: 2.4036 - val_acc: 0.3240\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 2s 333us/step - loss: 2.3761 - acc: 0.3572 - val_loss: 2.3672 - val_acc: 0.3450\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 2s 306us/step - loss: 2.3379 - acc: 0.3865 - val_loss: 2.3280 - val_acc: 0.3720\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 2s 296us/step - loss: 2.2968 - acc: 0.4164 - val_loss: 2.2862 - val_acc: 0.3990\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 2s 327us/step - loss: 2.2530 - acc: 0.4380 - val_loss: 2.2419 - val_acc: 0.4300\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 2s 312us/step - loss: 2.2065 - acc: 0.4680 - val_loss: 2.1950 - val_acc: 0.4390\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 2s 309us/step - loss: 2.1577 - acc: 0.4829 - val_loss: 2.1477 - val_acc: 0.4720\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 2s 310us/step - loss: 2.1077 - acc: 0.5016 - val_loss: 2.0988 - val_acc: 0.4910\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 2s 305us/step - loss: 2.0574 - acc: 0.5249 - val_loss: 2.0497 - val_acc: 0.5030\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 2s 331us/step - loss: 2.0071 - acc: 0.5441 - val_loss: 2.0030 - val_acc: 0.5350\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 2s 330us/step - loss: 1.9578 - acc: 0.5713 - val_loss: 1.9551 - val_acc: 0.5340\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 2s 329us/step - loss: 1.9107 - acc: 0.5799 - val_loss: 1.9113 - val_acc: 0.5690\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 2s 333us/step - loss: 1.8650 - acc: 0.6016 - val_loss: 1.8701 - val_acc: 0.5800\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 2s 323us/step - loss: 1.8219 - acc: 0.6141 - val_loss: 1.8304 - val_acc: 0.5960\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 3s 384us/step - loss: 1.7808 - acc: 0.6317 - val_loss: 1.7926 - val_acc: 0.6000\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 3s 340us/step - loss: 1.7420 - acc: 0.6429 - val_loss: 1.7563 - val_acc: 0.6160\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 2s 317us/step - loss: 1.7057 - acc: 0.6515 - val_loss: 1.7234 - val_acc: 0.6250\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 2s 311us/step - loss: 1.6715 - acc: 0.6604 - val_loss: 1.6929 - val_acc: 0.6280\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 2s 314us/step - loss: 1.6392 - acc: 0.6669 - val_loss: 1.6634 - val_acc: 0.6420\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 2s 315us/step - loss: 1.6093 - acc: 0.6768 - val_loss: 1.6391 - val_acc: 0.6430\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 2s 315us/step - loss: 1.5811 - acc: 0.6852 - val_loss: 1.6116 - val_acc: 0.6590\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 2s 321us/step - loss: 1.5542 - acc: 0.6903 - val_loss: 1.5890 - val_acc: 0.6530\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 2s 332us/step - loss: 1.5292 - acc: 0.6987 - val_loss: 1.5663 - val_acc: 0.6620\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 3s 341us/step - loss: 1.5060 - acc: 0.7012 - val_loss: 1.5461 - val_acc: 0.6610\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 3s 342us/step - loss: 1.4831 - acc: 0.7072 - val_loss: 1.5279 - val_acc: 0.6730\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 2s 310us/step - loss: 1.4632 - acc: 0.7132 - val_loss: 1.5117 - val_acc: 0.6850\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 2s 304us/step - loss: 1.4431 - acc: 0.7177 - val_loss: 1.4937 - val_acc: 0.6830\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 2s 308us/step - loss: 1.4243 - acc: 0.7235 - val_loss: 1.4763 - val_acc: 0.6810\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 2s 308us/step - loss: 1.4067 - acc: 0.7277 - val_loss: 1.4631 - val_acc: 0.6920\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 2s 328us/step - loss: 1.3899 - acc: 0.7311 - val_loss: 1.4484 - val_acc: 0.6980\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 2s 333us/step - loss: 1.3745 - acc: 0.7356 - val_loss: 1.4343 - val_acc: 0.7010\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 2s 333us/step - loss: 1.3592 - acc: 0.7385 - val_loss: 1.4219 - val_acc: 0.7110\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 2s 313us/step - loss: 1.3446 - acc: 0.7431 - val_loss: 1.4096 - val_acc: 0.7080\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 2s 308us/step - loss: 1.3306 - acc: 0.7464 - val_loss: 1.3980 - val_acc: 0.7130\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 2s 296us/step - loss: 1.3175 - acc: 0.7507 - val_loss: 1.3876 - val_acc: 0.7170\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 2s 304us/step - loss: 1.3047 - acc: 0.7527 - val_loss: 1.3784 - val_acc: 0.7210\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 2s 304us/step - loss: 1.2927 - acc: 0.7567 - val_loss: 1.3678 - val_acc: 0.7260\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 2s 302us/step - loss: 1.2809 - acc: 0.7575 - val_loss: 1.3575 - val_acc: 0.7230\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 2s 305us/step - loss: 1.2698 - acc: 0.7628 - val_loss: 1.3469 - val_acc: 0.7250\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 2s 297us/step - loss: 1.2590 - acc: 0.7639 - val_loss: 1.3393 - val_acc: 0.7290\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 2s 308us/step - loss: 1.2489 - acc: 0.7667 - val_loss: 1.3309 - val_acc: 0.7250\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 2s 329us/step - loss: 1.2385 - acc: 0.7685 - val_loss: 1.3224 - val_acc: 0.7280\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 2s 331us/step - loss: 1.2288 - acc: 0.7723 - val_loss: 1.3179 - val_acc: 0.7210\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 2s 322us/step - loss: 1.2195 - acc: 0.7747 - val_loss: 1.3091 - val_acc: 0.7300\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 2s 313us/step - loss: 1.2102 - acc: 0.7761 - val_loss: 1.3007 - val_acc: 0.7380\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 2s 311us/step - loss: 1.2009 - acc: 0.7812 - val_loss: 1.2948 - val_acc: 0.7380\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 2s 301us/step - loss: 1.1923 - acc: 0.7860 - val_loss: 1.2919 - val_acc: 0.7360\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 2s 301us/step - loss: 1.1844 - acc: 0.7839 - val_loss: 1.2820 - val_acc: 0.7350\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 2s 297us/step - loss: 1.1759 - acc: 0.7868 - val_loss: 1.2746 - val_acc: 0.7380\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 2s 324us/step - loss: 1.1675 - acc: 0.7883 - val_loss: 1.2681 - val_acc: 0.7460\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 2s 297us/step - loss: 1.1600 - acc: 0.7932 - val_loss: 1.2618 - val_acc: 0.7380\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 2s 292us/step - loss: 1.1525 - acc: 0.7961 - val_loss: 1.2594 - val_acc: 0.7480\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 2s 292us/step - loss: 1.1448 - acc: 0.7973 - val_loss: 1.2523 - val_acc: 0.7410\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 2s 292us/step - loss: 1.1377 - acc: 0.7960 - val_loss: 1.2463 - val_acc: 0.7400\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 2s 292us/step - loss: 1.1306 - acc: 0.8005 - val_loss: 1.2401 - val_acc: 0.7470\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 3s 344us/step - loss: 1.1237 - acc: 0.7991 - val_loss: 1.2365 - val_acc: 0.7550\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 3s 348us/step - loss: 1.1168 - acc: 0.8032 - val_loss: 1.2327 - val_acc: 0.7550\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 3s 361us/step - loss: 1.1102 - acc: 0.8028 - val_loss: 1.2255 - val_acc: 0.7510\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 2s 330us/step - loss: 1.1035 - acc: 0.8064 - val_loss: 1.2243 - val_acc: 0.7470\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 2s 328us/step - loss: 1.0974 - acc: 0.8087 - val_loss: 1.2217 - val_acc: 0.7460\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 3s 335us/step - loss: 1.0908 - acc: 0.8079 - val_loss: 1.2115 - val_acc: 0.7540\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 2s 314us/step - loss: 1.0847 - acc: 0.8108 - val_loss: 1.2106 - val_acc: 0.7510\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 2s 306us/step - loss: 1.0781 - acc: 0.8121 - val_loss: 1.2061 - val_acc: 0.7540\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 2s 327us/step - loss: 1.0721 - acc: 0.8163 - val_loss: 1.2029 - val_acc: 0.7620\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 3s 346us/step - loss: 1.0665 - acc: 0.8124 - val_loss: 1.1967 - val_acc: 0.7510\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.0602 - acc: 0.816 - 3s 349us/step - loss: 1.0608 - acc: 0.8160 - val_loss: 1.1912 - val_acc: 0.7590\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 3s 346us/step - loss: 1.0551 - acc: 0.8204 - val_loss: 1.1869 - val_acc: 0.7640\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 2s 327us/step - loss: 1.0493 - acc: 0.8191 - val_loss: 1.1844 - val_acc: 0.7520\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 2s 310us/step - loss: 1.0436 - acc: 0.8231 - val_loss: 1.1822 - val_acc: 0.7610\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 2s 309us/step - loss: 1.0383 - acc: 0.8229 - val_loss: 1.1770 - val_acc: 0.7660\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 2s 251us/step - loss: 1.0332 - acc: 0.8240 - val_loss: 1.1748 - val_acc: 0.7650\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 1.0274 - acc: 0.8253 - val_loss: 1.1703 - val_acc: 0.7630\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 2s 289us/step - loss: 1.0225 - acc: 0.8292 - val_loss: 1.1674 - val_acc: 0.7670\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 2s 295us/step - loss: 1.0173 - acc: 0.8309 - val_loss: 1.1628 - val_acc: 0.7650\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 2s 243us/step - loss: 1.0116 - acc: 0.8303 - val_loss: 1.1635 - val_acc: 0.7610\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 2s 244us/step - loss: 1.0069 - acc: 0.8331 - val_loss: 1.1585 - val_acc: 0.7620\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 2s 241us/step - loss: 1.0023 - acc: 0.8331 - val_loss: 1.1518 - val_acc: 0.7660\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 2s 253us/step - loss: 0.9973 - acc: 0.8363 - val_loss: 1.1520 - val_acc: 0.7610\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 2s 332us/step - loss: 0.9923 - acc: 0.8361 - val_loss: 1.1495 - val_acc: 0.7590\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 2s 324us/step - loss: 0.9877 - acc: 0.8353 - val_loss: 1.1434 - val_acc: 0.7650\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 2s 323us/step - loss: 0.9829 - acc: 0.8397 - val_loss: 1.1465 - val_acc: 0.7640\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 2s 324us/step - loss: 0.9783 - acc: 0.8401 - val_loss: 1.1362 - val_acc: 0.7630\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 2s 311us/step - loss: 0.9737 - acc: 0.8408 - val_loss: 1.1379 - val_acc: 0.7610\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 3s 341us/step - loss: 0.9685 - acc: 0.8411 - val_loss: 1.1337 - val_acc: 0.7670\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 2s 285us/step - loss: 0.9646 - acc: 0.8423 - val_loss: 1.1310 - val_acc: 0.7600\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.9602 - acc: 0.8436 - val_loss: 1.1266 - val_acc: 0.7610\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 2s 264us/step - loss: 0.9555 - acc: 0.8433 - val_loss: 1.1267 - val_acc: 0.7660\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 3s 345us/step - loss: 0.9514 - acc: 0.8463 - val_loss: 1.1208 - val_acc: 0.7650\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 2s 317us/step - loss: 0.9469 - acc: 0.8456 - val_loss: 1.1192 - val_acc: 0.7660\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 2s 320us/step - loss: 0.9427 - acc: 0.8483 - val_loss: 1.1158 - val_acc: 0.7620\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 2s 324us/step - loss: 0.9383 - acc: 0.8481 - val_loss: 1.1119 - val_acc: 0.7620\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 2s 316us/step - loss: 0.9342 - acc: 0.8496 - val_loss: 1.1123 - val_acc: 0.7630\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 2s 314us/step - loss: 0.9303 - acc: 0.8527 - val_loss: 1.1067 - val_acc: 0.7640\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 2s 314us/step - loss: 0.9259 - acc: 0.8545 - val_loss: 1.1050 - val_acc: 0.7620\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 2s 309us/step - loss: 0.9216 - acc: 0.8557 - val_loss: 1.1078 - val_acc: 0.7650\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 2s 319us/step - loss: 0.9176 - acc: 0.8532 - val_loss: 1.1012 - val_acc: 0.7590\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 2s 315us/step - loss: 0.9137 - acc: 0.8569 - val_loss: 1.1001 - val_acc: 0.7600\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 2s 325us/step - loss: 0.9099 - acc: 0.8579 - val_loss: 1.0991 - val_acc: 0.7600\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 3s 334us/step - loss: 0.9059 - acc: 0.8605 - val_loss: 1.0954 - val_acc: 0.7630\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 2s 314us/step - loss: 0.9020 - acc: 0.8588 - val_loss: 1.0913 - val_acc: 0.7670\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 2s 310us/step - loss: 0.8982 - acc: 0.8611 - val_loss: 1.0902 - val_acc: 0.7610\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 2s 309us/step - loss: 0.8944 - acc: 0.8624 - val_loss: 1.0878 - val_acc: 0.7660\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 2s 309us/step - loss: 0.8907 - acc: 0.8631 - val_loss: 1.0911 - val_acc: 0.7610\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 2s 303us/step - loss: 0.8868 - acc: 0.8636 - val_loss: 1.0864 - val_acc: 0.7640\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 2s 309us/step - loss: 0.8832 - acc: 0.8663 - val_loss: 1.0818 - val_acc: 0.7600\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 2s 310us/step - loss: 0.8794 - acc: 0.8661 - val_loss: 1.0837 - val_acc: 0.7620\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 2s 310us/step - loss: 0.8761 - acc: 0.8664 - val_loss: 1.0793 - val_acc: 0.7640\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 2s 311us/step - loss: 0.8725 - acc: 0.8685 - val_loss: 1.0768 - val_acc: 0.7660\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 2s 317us/step - loss: 0.8686 - acc: 0.8691 - val_loss: 1.0739 - val_acc: 0.7610\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 2s 314us/step - loss: 0.8653 - acc: 0.8705 - val_loss: 1.0750 - val_acc: 0.7630\n",
      "Epoch 119/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 2s 325us/step - loss: 0.8616 - acc: 0.8689 - val_loss: 1.0725 - val_acc: 0.7630\n",
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 2s 319us/step - loss: 0.8580 - acc: 0.8719 - val_loss: 1.0719 - val_acc: 0.7640\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how regularization has affected our model results.  \n",
    "\n",
    "Run the cell below to get the model's `.history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs).\n",
    "\n",
    "Run the cell below to visualize our training and validation accuracy both with and without L2 regularization, so that we can compare them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the same model we did above, but this time, set the `kernel_regularizer` to `regularizers.l1(0.005)` inside both hidden layers. \n",
    "* Compile and fit the model exactly as we did for our L2 Regularization experiment (`120` epochs) \n",
    "* Store the fitted model that the `.fit` call returns inside a variable called `L1_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 3s 388us/step - loss: 15.9829 - acc: 0.1499 - val_loss: 15.5729 - val_acc: 0.1700\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 2s 292us/step - loss: 15.2221 - acc: 0.1780 - val_loss: 14.8274 - val_acc: 0.2010\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 2s 294us/step - loss: 14.4838 - acc: 0.2095 - val_loss: 14.1022 - val_acc: 0.2320\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 2s 298us/step - loss: 13.7654 - acc: 0.2472 - val_loss: 13.3964 - val_acc: 0.2690\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 2s 296us/step - loss: 13.0659 - acc: 0.2797 - val_loss: 12.7093 - val_acc: 0.3000\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 2s 295us/step - loss: 12.3848 - acc: 0.3164 - val_loss: 12.0406 - val_acc: 0.3500\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 2s 304us/step - loss: 11.7223 - acc: 0.3584 - val_loss: 11.3909 - val_acc: 0.3670\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 2s 305us/step - loss: 11.0797 - acc: 0.3885 - val_loss: 10.7611 - val_acc: 0.3830\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 2s 307us/step - loss: 10.4578 - acc: 0.4092 - val_loss: 10.1529 - val_acc: 0.4190\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 2s 294us/step - loss: 9.8572 - acc: 0.4413 - val_loss: 9.5660 - val_acc: 0.4340\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 2s 264us/step - loss: 9.2786 - acc: 0.4655 - val_loss: 9.0024 - val_acc: 0.4540\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 2s 257us/step - loss: 8.7231 - acc: 0.4813 - val_loss: 8.4602 - val_acc: 0.4770\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 2s 243us/step - loss: 8.1909 - acc: 0.5045 - val_loss: 7.9408 - val_acc: 0.4790\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 2s 242us/step - loss: 7.6810 - acc: 0.5213 - val_loss: 7.4451 - val_acc: 0.5010\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 7.1948 - acc: 0.5445 - val_loss: 6.9730 - val_acc: 0.5190\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 2s 258us/step - loss: 6.7309 - acc: 0.5573 - val_loss: 6.5225 - val_acc: 0.5310\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 2s 254us/step - loss: 6.2902 - acc: 0.5729 - val_loss: 6.0935 - val_acc: 0.5470\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 2s 250us/step - loss: 5.8723 - acc: 0.5853 - val_loss: 5.6901 - val_acc: 0.5570\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 2s 254us/step - loss: 5.4772 - acc: 0.6029 - val_loss: 5.3075 - val_acc: 0.5580\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 2s 256us/step - loss: 5.1046 - acc: 0.6083 - val_loss: 4.9469 - val_acc: 0.5700\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 2s 262us/step - loss: 4.7540 - acc: 0.6239 - val_loss: 4.6088 - val_acc: 0.5740\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 2s 329us/step - loss: 4.4264 - acc: 0.6285 - val_loss: 4.2938 - val_acc: 0.5760\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 2s 320us/step - loss: 4.1210 - acc: 0.6300 - val_loss: 4.0014 - val_acc: 0.5910\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 3s 345us/step - loss: 3.8385 - acc: 0.6384 - val_loss: 3.7326 - val_acc: 0.5850\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 3s 335us/step - loss: 3.5783 - acc: 0.6411 - val_loss: 3.4834 - val_acc: 0.5940\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 2s 275us/step - loss: 3.3400 - acc: 0.6409 - val_loss: 3.2568 - val_acc: 0.6060\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 3.1226 - acc: 0.6469 - val_loss: 3.0509 - val_acc: 0.6040\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 2s 244us/step - loss: 2.9266 - acc: 0.6467 - val_loss: 2.8678 - val_acc: 0.6130\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 2s 244us/step - loss: 2.7524 - acc: 0.6480 - val_loss: 2.7053 - val_acc: 0.6050\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 2.5989 - acc: 0.6489 - val_loss: 2.5648 - val_acc: 0.6170\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 2.4663 - acc: 0.6516 - val_loss: 2.4409 - val_acc: 0.6150\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 2s 244us/step - loss: 2.3532 - acc: 0.6532 - val_loss: 2.3392 - val_acc: 0.6140\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 2s 243us/step - loss: 2.2604 - acc: 0.6535 - val_loss: 2.2568 - val_acc: 0.6150\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 2s 251us/step - loss: 2.1853 - acc: 0.6525 - val_loss: 2.1921 - val_acc: 0.6190\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 2s 266us/step - loss: 2.1275 - acc: 0.6545 - val_loss: 2.1438 - val_acc: 0.6180\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 2.0844 - acc: 0.6561 - val_loss: 2.1063 - val_acc: 0.6110\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 2s 251us/step - loss: 2.0520 - acc: 0.6555 - val_loss: 2.0776 - val_acc: 0.6160\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 2s 324us/step - loss: 2.0261 - acc: 0.6551 - val_loss: 2.0558 - val_acc: 0.6210\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 2s 332us/step - loss: 2.0040 - acc: 0.6564 - val_loss: 2.0369 - val_acc: 0.6240\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 2s 305us/step - loss: 1.9843 - acc: 0.6592 - val_loss: 2.0192 - val_acc: 0.6290\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 2s 318us/step - loss: 1.9661 - acc: 0.6609 - val_loss: 2.0006 - val_acc: 0.6230\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 2s 322us/step - loss: 1.9489 - acc: 0.6615 - val_loss: 1.9838 - val_acc: 0.6260\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 2s 265us/step - loss: 1.9324 - acc: 0.6616 - val_loss: 1.9682 - val_acc: 0.6310\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 2s 296us/step - loss: 1.9172 - acc: 0.6632 - val_loss: 1.9533 - val_acc: 0.6290\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 2s 306us/step - loss: 1.9024 - acc: 0.6644 - val_loss: 1.9408 - val_acc: 0.6440\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 2s 263us/step - loss: 1.8883 - acc: 0.6685 - val_loss: 1.9272 - val_acc: 0.6350\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 2s 257us/step - loss: 1.8751 - acc: 0.6700 - val_loss: 1.9146 - val_acc: 0.6460\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 1.8620 - acc: 0.6711 - val_loss: 1.9020 - val_acc: 0.6430\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 2s 304us/step - loss: 1.8494 - acc: 0.6709 - val_loss: 1.8907 - val_acc: 0.6460\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 2s 305us/step - loss: 1.8370 - acc: 0.6737 - val_loss: 1.8788 - val_acc: 0.6430\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 2s 324us/step - loss: 1.8256 - acc: 0.6737 - val_loss: 1.8697 - val_acc: 0.6470\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 2s 328us/step - loss: 1.8138 - acc: 0.6753 - val_loss: 1.8575 - val_acc: 0.6460\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 2s 303us/step - loss: 1.8025 - acc: 0.6788 - val_loss: 1.8442 - val_acc: 0.6470\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 2s 305us/step - loss: 1.7912 - acc: 0.6768 - val_loss: 1.8380 - val_acc: 0.6510\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 2s 314us/step - loss: 1.7810 - acc: 0.6769 - val_loss: 1.8260 - val_acc: 0.6490\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 2s 307us/step - loss: 1.7709 - acc: 0.6817 - val_loss: 1.8143 - val_acc: 0.6510\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 2s 301us/step - loss: 1.7600 - acc: 0.6813 - val_loss: 1.8045 - val_acc: 0.6570\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 2s 302us/step - loss: 1.7498 - acc: 0.6808 - val_loss: 1.7954 - val_acc: 0.6560\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 2s 319us/step - loss: 1.7395 - acc: 0.6833 - val_loss: 1.7868 - val_acc: 0.6490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/120\n",
      "7500/7500 [==============================] - 2s 332us/step - loss: 1.7298 - acc: 0.6849 - val_loss: 1.7755 - val_acc: 0.6540\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 2s 319us/step - loss: 1.7207 - acc: 0.6843 - val_loss: 1.7669 - val_acc: 0.6530\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 2s 319us/step - loss: 1.7112 - acc: 0.6853 - val_loss: 1.7716 - val_acc: 0.6470\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 2s 321us/step - loss: 1.7028 - acc: 0.6843 - val_loss: 1.7474 - val_acc: 0.6600\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 2s 311us/step - loss: 1.6929 - acc: 0.6880 - val_loss: 1.7388 - val_acc: 0.6630\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 2s 296us/step - loss: 1.6847 - acc: 0.6879 - val_loss: 1.7306 - val_acc: 0.6570\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 2s 302us/step - loss: 1.6752 - acc: 0.6904 - val_loss: 1.7238 - val_acc: 0.6600\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 2s 301us/step - loss: 1.6669 - acc: 0.6905 - val_loss: 1.7142 - val_acc: 0.6580\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 2s 296us/step - loss: 1.6589 - acc: 0.6912 - val_loss: 1.7050 - val_acc: 0.6560\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 2s 298us/step - loss: 1.6500 - acc: 0.6904 - val_loss: 1.6996 - val_acc: 0.6600\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 2s 320us/step - loss: 1.6418 - acc: 0.6931 - val_loss: 1.6893 - val_acc: 0.6680\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 3s 343us/step - loss: 1.6337 - acc: 0.6921 - val_loss: 1.6819 - val_acc: 0.6660\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 2s 316us/step - loss: 1.6260 - acc: 0.6935 - val_loss: 1.6734 - val_acc: 0.6690\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 2s 314us/step - loss: 1.6180 - acc: 0.6913 - val_loss: 1.6661 - val_acc: 0.6710\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 2s 291us/step - loss: 1.6098 - acc: 0.6940 - val_loss: 1.6577 - val_acc: 0.6680\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 2s 270us/step - loss: 1.6026 - acc: 0.6948 - val_loss: 1.6521 - val_acc: 0.6650\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 2s 254us/step - loss: 1.5951 - acc: 0.6943 - val_loss: 1.6436 - val_acc: 0.6670\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 2s 244us/step - loss: 1.5874 - acc: 0.6949 - val_loss: 1.6374 - val_acc: 0.6660\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 2s 254us/step - loss: 1.5802 - acc: 0.6969 - val_loss: 1.6282 - val_acc: 0.6680\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 2s 260us/step - loss: 1.5725 - acc: 0.6971 - val_loss: 1.6216 - val_acc: 0.6700\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 2s 260us/step - loss: 1.5654 - acc: 0.6979 - val_loss: 1.6157 - val_acc: 0.6690\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 2s 312us/step - loss: 1.5584 - acc: 0.6981 - val_loss: 1.6102 - val_acc: 0.6670\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 2s 273us/step - loss: 1.5509 - acc: 0.6984 - val_loss: 1.6009 - val_acc: 0.6700\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 2s 321us/step - loss: 1.5444 - acc: 0.6984 - val_loss: 1.6052 - val_acc: 0.6690\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 2s 332us/step - loss: 1.5379 - acc: 0.7000 - val_loss: 1.5866 - val_acc: 0.6710\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 2s 266us/step - loss: 1.5301 - acc: 0.7007 - val_loss: 1.5825 - val_acc: 0.6710\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 2s 263us/step - loss: 1.5236 - acc: 0.6996 - val_loss: 1.5748 - val_acc: 0.6700\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 2s 271us/step - loss: 1.5172 - acc: 0.7007 - val_loss: 1.5705 - val_acc: 0.6700\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 2s 278us/step - loss: 1.5107 - acc: 0.7004 - val_loss: 1.5602 - val_acc: 0.6710\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 2s 317us/step - loss: 1.5041 - acc: 0.7012 - val_loss: 1.5555 - val_acc: 0.6750\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 2s 318us/step - loss: 1.4975 - acc: 0.7035 - val_loss: 1.5482 - val_acc: 0.6740\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 2s 301us/step - loss: 1.4913 - acc: 0.7045 - val_loss: 1.5478 - val_acc: 0.6720\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 2s 312us/step - loss: 1.4848 - acc: 0.7045 - val_loss: 1.5354 - val_acc: 0.6740\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 3s 348us/step - loss: 1.4790 - acc: 0.7041 - val_loss: 1.5300 - val_acc: 0.6740\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 3s 440us/step - loss: 1.4727 - acc: 0.7040 - val_loss: 1.5211 - val_acc: 0.6760\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 3s 347us/step - loss: 1.4661 - acc: 0.7055 - val_loss: 1.5167 - val_acc: 0.6740\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 2s 327us/step - loss: 1.4605 - acc: 0.7048 - val_loss: 1.5131 - val_acc: 0.6800\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 2s 309us/step - loss: 1.4546 - acc: 0.7059 - val_loss: 1.5042 - val_acc: 0.6800\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 2s 314us/step - loss: 1.4487 - acc: 0.7060 - val_loss: 1.4986 - val_acc: 0.6830\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 2s 307us/step - loss: 1.4430 - acc: 0.7081 - val_loss: 1.4928 - val_acc: 0.6750\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 2s 333us/step - loss: 1.4373 - acc: 0.7064 - val_loss: 1.4875 - val_acc: 0.6800\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 2s 265us/step - loss: 1.4317 - acc: 0.7081 - val_loss: 1.4869 - val_acc: 0.6770\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 2s 262us/step - loss: 1.4264 - acc: 0.7084 - val_loss: 1.4765 - val_acc: 0.6790\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 2s 260us/step - loss: 1.4209 - acc: 0.7099 - val_loss: 1.4770 - val_acc: 0.6760\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 2s 274us/step - loss: 1.4150 - acc: 0.7103 - val_loss: 1.4677 - val_acc: 0.6790\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 2s 280us/step - loss: 1.4097 - acc: 0.7107 - val_loss: 1.4610 - val_acc: 0.6810\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 2s 266us/step - loss: 1.4043 - acc: 0.7091 - val_loss: 1.4546 - val_acc: 0.6820\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 2s 273us/step - loss: 1.3989 - acc: 0.7123 - val_loss: 1.4508 - val_acc: 0.6830\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 2s 274us/step - loss: 1.3939 - acc: 0.7128 - val_loss: 1.4425 - val_acc: 0.6850\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 2s 274us/step - loss: 1.3888 - acc: 0.7119 - val_loss: 1.4401 - val_acc: 0.6820\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 2s 260us/step - loss: 1.3842 - acc: 0.7121 - val_loss: 1.4344 - val_acc: 0.6790\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 2s 276us/step - loss: 1.3787 - acc: 0.7120 - val_loss: 1.4299 - val_acc: 0.6790\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 2s 261us/step - loss: 1.3731 - acc: 0.7132 - val_loss: 1.4295 - val_acc: 0.6830\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 2s 254us/step - loss: 1.3685 - acc: 0.7144 - val_loss: 1.4178 - val_acc: 0.6870\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 2s 256us/step - loss: 1.3635 - acc: 0.7151 - val_loss: 1.4170 - val_acc: 0.6830\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 1.3595 - acc: 0.7148 - val_loss: 1.4098 - val_acc: 0.6840\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 1.3536 - acc: 0.7145 - val_loss: 1.4039 - val_acc: 0.6850\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 2s 245us/step - loss: 1.3489 - acc: 0.7149 - val_loss: 1.4071 - val_acc: 0.6820\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 2s 251us/step - loss: 1.3445 - acc: 0.7143 - val_loss: 1.3947 - val_acc: 0.6890\n",
      "Epoch 119/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 2s 253us/step - loss: 1.3397 - acc: 0.7165 - val_loss: 1.3921 - val_acc: 0.6890\n",
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 2s 250us/step - loss: 1.3350 - acc: 0.7164 - val_loss: 1.3851 - val_acc: 0.6870\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to get and visualize the model's `.history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer.\n",
    "\n",
    "To complete our comparison, let's use `model.evaluate()` again on the appropriate variables to compare results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 2s 273us/step\n",
      "1500/1500 [==============================] - 0s 276us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.334806237188975, 0.7164]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Output: [1.3186310468037923, 0.72266666663487755]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3708883482615153, 0.7026666666666667]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Output: [1.3541648308436076, 0.70800000031789145]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dropout Regularization\n",
    "\n",
    "Dropout Regularization is accomplished by adding in an additional `Dropout` layer wherever we want to use it, and providing a percentage value for how likely any given neuron is to get \"dropped out\" during this layer. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import `Dropout` from `keras.layers`\n",
    "* Recreate the same network we have above, but this time without any L1 or L2 regularization\n",
    "* Add a `Dropout` layer between hidden layer 1 and hidden layer 2.  This should have a dropout chance of `0.3`.\n",
    "* Add a `Dropout` layer between hidden layer 2 and the output layer.  This should have a dropout chance of `0.3`.\n",
    "* Compile the model with the exact same hyperparameters as all other models we've built. \n",
    "* Fit the model with the same hyperparameters we've used above.  But this time, train the model for `200` epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 3s 401us/step - loss: 1.9785 - acc: 0.1415 - val_loss: 1.9415 - val_acc: 0.1650\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 2s 328us/step - loss: 1.9480 - acc: 0.1603 - val_loss: 1.9209 - val_acc: 0.1870\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 3s 339us/step - loss: 1.9292 - acc: 0.1793 - val_loss: 1.9043 - val_acc: 0.2110\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 3s 341us/step - loss: 1.9188 - acc: 0.1859 - val_loss: 1.8883 - val_acc: 0.2160\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 3s 341us/step - loss: 1.8996 - acc: 0.2035 - val_loss: 1.8730 - val_acc: 0.2300\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 3s 348us/step - loss: 1.8861 - acc: 0.2160 - val_loss: 1.8573 - val_acc: 0.2420\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 2s 311us/step - loss: 1.8732 - acc: 0.2253 - val_loss: 1.8420 - val_acc: 0.2610\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 3s 342us/step - loss: 1.8610 - acc: 0.2404 - val_loss: 1.8257 - val_acc: 0.2860\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 3s 345us/step - loss: 1.8472 - acc: 0.2537 - val_loss: 1.8071 - val_acc: 0.3060\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 3s 340us/step - loss: 1.8286 - acc: 0.2609 - val_loss: 1.7867 - val_acc: 0.3270\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 3s 348us/step - loss: 1.8134 - acc: 0.2689 - val_loss: 1.7683 - val_acc: 0.3570\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 2s 322us/step - loss: 1.7932 - acc: 0.2877 - val_loss: 1.7456 - val_acc: 0.3790\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 2s 309us/step - loss: 1.7793 - acc: 0.2916 - val_loss: 1.7231 - val_acc: 0.3910\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 2s 306us/step - loss: 1.7554 - acc: 0.3093 - val_loss: 1.6996 - val_acc: 0.4090\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 2s 308us/step - loss: 1.7316 - acc: 0.3227 - val_loss: 1.6747 - val_acc: 0.4280\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 2s 307us/step - loss: 1.7128 - acc: 0.3405 - val_loss: 1.6504 - val_acc: 0.4420\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 2s 307us/step - loss: 1.6988 - acc: 0.3375 - val_loss: 1.6256 - val_acc: 0.4670\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 2s 299us/step - loss: 1.6840 - acc: 0.3527 - val_loss: 1.6023 - val_acc: 0.4810\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 2s 301us/step - loss: 1.6624 - acc: 0.3645 - val_loss: 1.5760 - val_acc: 0.5060\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 2s 300us/step - loss: 1.6293 - acc: 0.3829 - val_loss: 1.5480 - val_acc: 0.5180\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 2s 301us/step - loss: 1.6093 - acc: 0.3829 - val_loss: 1.5206 - val_acc: 0.5280\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 3s 344us/step - loss: 1.5900 - acc: 0.4007 - val_loss: 1.4951 - val_acc: 0.5370\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 2s 330us/step - loss: 1.5719 - acc: 0.3988 - val_loss: 1.4695 - val_acc: 0.5430\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 2s 306us/step - loss: 1.5602 - acc: 0.4032 - val_loss: 1.4464 - val_acc: 0.5570\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 2s 300us/step - loss: 1.5351 - acc: 0.4116 - val_loss: 1.4195 - val_acc: 0.5670\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 2s 281us/step - loss: 1.5038 - acc: 0.4344 - val_loss: 1.3938 - val_acc: 0.5780\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 2s 289us/step - loss: 1.4882 - acc: 0.4431 - val_loss: 1.3683 - val_acc: 0.5940\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 2s 293us/step - loss: 1.4652 - acc: 0.4453 - val_loss: 1.3441 - val_acc: 0.6020\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 2s 261us/step - loss: 1.4520 - acc: 0.4479 - val_loss: 1.3232 - val_acc: 0.6030\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 2s 252us/step - loss: 1.4313 - acc: 0.4548 - val_loss: 1.3005 - val_acc: 0.6050\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 2s 260us/step - loss: 1.4122 - acc: 0.4585 - val_loss: 1.2784 - val_acc: 0.6130\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 2s 271us/step - loss: 1.3873 - acc: 0.4775 - val_loss: 1.2585 - val_acc: 0.6200\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 2s 282us/step - loss: 1.3772 - acc: 0.4752 - val_loss: 1.2385 - val_acc: 0.6270\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 2s 262us/step - loss: 1.3596 - acc: 0.4885 - val_loss: 1.2180 - val_acc: 0.6280\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 2s 254us/step - loss: 1.3392 - acc: 0.4915 - val_loss: 1.2002 - val_acc: 0.6370\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 2s 250us/step - loss: 1.3146 - acc: 0.5075 - val_loss: 1.1782 - val_acc: 0.6340\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 2s 256us/step - loss: 1.3037 - acc: 0.5075 - val_loss: 1.1611 - val_acc: 0.6460\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 2s 252us/step - loss: 1.2952 - acc: 0.5104 - val_loss: 1.1438 - val_acc: 0.6550\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 2s 257us/step - loss: 1.2828 - acc: 0.5193 - val_loss: 1.1282 - val_acc: 0.6580\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 2s 267us/step - loss: 1.2702 - acc: 0.5211 - val_loss: 1.1148 - val_acc: 0.6640\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 2s 256us/step - loss: 1.2536 - acc: 0.5252 - val_loss: 1.0972 - val_acc: 0.6680\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 2s 267us/step - loss: 1.2293 - acc: 0.5437 - val_loss: 1.0827 - val_acc: 0.6690\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 2s 265us/step - loss: 1.2219 - acc: 0.5429 - val_loss: 1.0680 - val_acc: 0.6750\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 2s 269us/step - loss: 1.2191 - acc: 0.5409 - val_loss: 1.0559 - val_acc: 0.6760\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 1.1956 - acc: 0.5555 - val_loss: 1.0394 - val_acc: 0.6870\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 1.1845 - acc: 0.5617 - val_loss: 1.0274 - val_acc: 0.6870\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 1.1637 - acc: 0.5679 - val_loss: 1.0113 - val_acc: 0.6930\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 1.1530 - acc: 0.5708 - val_loss: 0.9975 - val_acc: 0.6970\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 1.1535 - acc: 0.5653 - val_loss: 0.9906 - val_acc: 0.6950\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 1.1301 - acc: 0.5773 - val_loss: 0.9776 - val_acc: 0.7050\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 1.1118 - acc: 0.5863 - val_loss: 0.9646 - val_acc: 0.7030\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 1.1039 - acc: 0.5881 - val_loss: 0.9535 - val_acc: 0.7050\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 2s 249us/step - loss: 1.0970 - acc: 0.5852 - val_loss: 0.9430 - val_acc: 0.7090\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 1.0924 - acc: 0.5863 - val_loss: 0.9340 - val_acc: 0.7090\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 1.0724 - acc: 0.5971 - val_loss: 0.9244 - val_acc: 0.7150\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 1.0774 - acc: 0.5940 - val_loss: 0.9168 - val_acc: 0.7130\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 2s 262us/step - loss: 1.0583 - acc: 0.6032 - val_loss: 0.9065 - val_acc: 0.7180\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 2s 331us/step - loss: 1.0547 - acc: 0.6088 - val_loss: 0.8999 - val_acc: 0.7180\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 2s 324us/step - loss: 1.0403 - acc: 0.6105 - val_loss: 0.8908 - val_acc: 0.7200\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 2s 312us/step - loss: 1.0304 - acc: 0.6132 - val_loss: 0.8833 - val_acc: 0.7190\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 2s 291us/step - loss: 1.0255 - acc: 0.6207 - val_loss: 0.8748 - val_acc: 0.7180\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 2s 287us/step - loss: 1.0180 - acc: 0.6169 - val_loss: 0.8672 - val_acc: 0.7260\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 2s 294us/step - loss: 1.0112 - acc: 0.6219 - val_loss: 0.8600 - val_acc: 0.7280\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 2s 280us/step - loss: 1.0010 - acc: 0.6276 - val_loss: 0.8547 - val_acc: 0.7230\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 2s 261us/step - loss: 0.9893 - acc: 0.6333 - val_loss: 0.8479 - val_acc: 0.7280\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 2s 270us/step - loss: 0.9739 - acc: 0.6399 - val_loss: 0.8409 - val_acc: 0.7310\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 2s 295us/step - loss: 0.9765 - acc: 0.6364 - val_loss: 0.8347 - val_acc: 0.7310\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 2s 287us/step - loss: 0.9741 - acc: 0.6332 - val_loss: 0.8261 - val_acc: 0.7300\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 2s 300us/step - loss: 0.9596 - acc: 0.6468 - val_loss: 0.8205 - val_acc: 0.7290\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 2s 299us/step - loss: 0.9399 - acc: 0.6473 - val_loss: 0.8139 - val_acc: 0.7310\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 3s 352us/step - loss: 0.9444 - acc: 0.6439 - val_loss: 0.8087 - val_acc: 0.7290\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 3s 342us/step - loss: 0.9384 - acc: 0.6511 - val_loss: 0.8031 - val_acc: 0.7320\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 2s 305us/step - loss: 0.9360 - acc: 0.6481 - val_loss: 0.7987 - val_acc: 0.7300\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 2s 332us/step - loss: 0.9259 - acc: 0.6567 - val_loss: 0.7954 - val_acc: 0.7290\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 2s 303us/step - loss: 0.9222 - acc: 0.6555 - val_loss: 0.7892 - val_acc: 0.7350\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 2s 301us/step - loss: 0.9246 - acc: 0.6557 - val_loss: 0.7864 - val_acc: 0.7290\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 2s 302us/step - loss: 0.9101 - acc: 0.6587 - val_loss: 0.7819 - val_acc: 0.7310\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 2s 309us/step - loss: 0.8973 - acc: 0.6597 - val_loss: 0.7750 - val_acc: 0.7330\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 2s 316us/step - loss: 0.9011 - acc: 0.6635 - val_loss: 0.7724 - val_acc: 0.7320\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 2s 308us/step - loss: 0.8885 - acc: 0.6713 - val_loss: 0.7682 - val_acc: 0.7320\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 2s 256us/step - loss: 0.8833 - acc: 0.6740 - val_loss: 0.7654 - val_acc: 0.7330\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.8838 - acc: 0.6663 - val_loss: 0.7624 - val_acc: 0.7310\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 2s 252us/step - loss: 0.8849 - acc: 0.6720 - val_loss: 0.7578 - val_acc: 0.7330\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 2s 256us/step - loss: 0.8570 - acc: 0.6808 - val_loss: 0.7518 - val_acc: 0.7360\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.8615 - acc: 0.6727 - val_loss: 0.7470 - val_acc: 0.7360\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.8536 - acc: 0.6765 - val_loss: 0.7451 - val_acc: 0.7370\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.8564 - acc: 0.6799 - val_loss: 0.7436 - val_acc: 0.7330\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.8462 - acc: 0.6815 - val_loss: 0.7384 - val_acc: 0.7390\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 2s 256us/step - loss: 0.8343 - acc: 0.6825 - val_loss: 0.7347 - val_acc: 0.7370\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 2s 254us/step - loss: 0.8344 - acc: 0.6885 - val_loss: 0.7315 - val_acc: 0.7390\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 2s 259us/step - loss: 0.8300 - acc: 0.6877 - val_loss: 0.7281 - val_acc: 0.7380\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 2s 252us/step - loss: 0.8289 - acc: 0.6927 - val_loss: 0.7263 - val_acc: 0.7390\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 2s 268us/step - loss: 0.8197 - acc: 0.6887 - val_loss: 0.7232 - val_acc: 0.7400\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 2s 266us/step - loss: 0.8173 - acc: 0.6959 - val_loss: 0.7190 - val_acc: 0.7400\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 2s 269us/step - loss: 0.8134 - acc: 0.6941 - val_loss: 0.7180 - val_acc: 0.7400\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 2s 276us/step - loss: 0.8122 - acc: 0.6975 - val_loss: 0.7156 - val_acc: 0.7400\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 2s 259us/step - loss: 0.8016 - acc: 0.6940 - val_loss: 0.7122 - val_acc: 0.7410\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.7996 - acc: 0.7003 - val_loss: 0.7107 - val_acc: 0.7410\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 2s 258us/step - loss: 0.7919 - acc: 0.6988 - val_loss: 0.7078 - val_acc: 0.7430\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 2s 253us/step - loss: 0.7912 - acc: 0.7029 - val_loss: 0.7059 - val_acc: 0.7420\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 2s 251us/step - loss: 0.7898 - acc: 0.7037 - val_loss: 0.7037 - val_acc: 0.7430\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 2s 262us/step - loss: 0.7830 - acc: 0.7032 - val_loss: 0.7014 - val_acc: 0.7440\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 3s 346us/step - loss: 0.7708 - acc: 0.7120 - val_loss: 0.6991 - val_acc: 0.7430\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 2s 324us/step - loss: 0.7729 - acc: 0.7119 - val_loss: 0.6970 - val_acc: 0.7430\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 3s 338us/step - loss: 0.7736 - acc: 0.7045 - val_loss: 0.6968 - val_acc: 0.7410\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 2s 316us/step - loss: 0.7718 - acc: 0.7095 - val_loss: 0.6946 - val_acc: 0.7440\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 2s 322us/step - loss: 0.7625 - acc: 0.7148 - val_loss: 0.6936 - val_acc: 0.7450\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 3s 339us/step - loss: 0.7674 - acc: 0.7095 - val_loss: 0.6909 - val_acc: 0.7490\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 2s 320us/step - loss: 0.7560 - acc: 0.7215 - val_loss: 0.6900 - val_acc: 0.7450\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 3s 336us/step - loss: 0.7512 - acc: 0.7165 - val_loss: 0.6883 - val_acc: 0.7460\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 3s 338us/step - loss: 0.7477 - acc: 0.7205 - val_loss: 0.6859 - val_acc: 0.7440\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 2s 301us/step - loss: 0.7405 - acc: 0.7217 - val_loss: 0.6842 - val_acc: 0.7470\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.7405 - acc: 0.7207 - val_loss: 0.6832 - val_acc: 0.7490\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.7446 - acc: 0.7181 - val_loss: 0.6817 - val_acc: 0.7500\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.7435 - acc: 0.7181 - val_loss: 0.6801 - val_acc: 0.7470\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.7324 - acc: 0.7259 - val_loss: 0.6796 - val_acc: 0.7480\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.7330 - acc: 0.7232 - val_loss: 0.6777 - val_acc: 0.7470\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.7299 - acc: 0.7273 - val_loss: 0.6768 - val_acc: 0.7520\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.7315 - acc: 0.7313 - val_loss: 0.6738 - val_acc: 0.7470\n",
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.7255 - acc: 0.7273 - val_loss: 0.6744 - val_acc: 0.7490\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 2s 249us/step - loss: 0.7289 - acc: 0.7252 - val_loss: 0.6722 - val_acc: 0.7490\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.7130 - acc: 0.7296 - val_loss: 0.6695 - val_acc: 0.7520\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.7055 - acc: 0.7323 - val_loss: 0.6688 - val_acc: 0.7530\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 2s 249us/step - loss: 0.7132 - acc: 0.7372 - val_loss: 0.6684 - val_acc: 0.7490\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.6971 - acc: 0.7383 - val_loss: 0.6664 - val_acc: 0.7480\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.6987 - acc: 0.7389 - val_loss: 0.6655 - val_acc: 0.7520\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.7035 - acc: 0.7328 - val_loss: 0.6643 - val_acc: 0.7530\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.7068 - acc: 0.7368 - val_loss: 0.6624 - val_acc: 0.7540\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.6947 - acc: 0.7389 - val_loss: 0.6618 - val_acc: 0.7530\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 2s 245us/step - loss: 0.6899 - acc: 0.7424 - val_loss: 0.6606 - val_acc: 0.7500\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.6898 - acc: 0.7415 - val_loss: 0.6600 - val_acc: 0.7520\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 2s 249us/step - loss: 0.6873 - acc: 0.7403 - val_loss: 0.6573 - val_acc: 0.7530\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.6819 - acc: 0.7383 - val_loss: 0.6576 - val_acc: 0.7540\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.6836 - acc: 0.7429 - val_loss: 0.6566 - val_acc: 0.7500\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 2s 252us/step - loss: 0.6763 - acc: 0.7407 - val_loss: 0.6560 - val_acc: 0.7520\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 2s 253us/step - loss: 0.6784 - acc: 0.7453 - val_loss: 0.6557 - val_acc: 0.7490\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.6707 - acc: 0.7451 - val_loss: 0.6564 - val_acc: 0.7550\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.6689 - acc: 0.7431 - val_loss: 0.6535 - val_acc: 0.7530\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.6797 - acc: 0.7412 - val_loss: 0.6532 - val_acc: 0.7520\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 2s 250us/step - loss: 0.6660 - acc: 0.7489 - val_loss: 0.6531 - val_acc: 0.7510\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.6631 - acc: 0.7517 - val_loss: 0.6516 - val_acc: 0.7520\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.6660 - acc: 0.7484 - val_loss: 0.6506 - val_acc: 0.7520\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.6563 - acc: 0.7513 - val_loss: 0.6500 - val_acc: 0.7540\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.6499 - acc: 0.7524 - val_loss: 0.6487 - val_acc: 0.7520\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.6571 - acc: 0.7511 - val_loss: 0.6496 - val_acc: 0.7500\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.6482 - acc: 0.7507 - val_loss: 0.6478 - val_acc: 0.7510\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.6500 - acc: 0.7517 - val_loss: 0.6469 - val_acc: 0.7510\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.6412 - acc: 0.7584 - val_loss: 0.6462 - val_acc: 0.7530\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.6454 - acc: 0.7529 - val_loss: 0.6463 - val_acc: 0.7540\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 2s 257us/step - loss: 0.6315 - acc: 0.7543 - val_loss: 0.6447 - val_acc: 0.7540\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 2s 255us/step - loss: 0.6356 - acc: 0.7561 - val_loss: 0.6444 - val_acc: 0.7500\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 2s 255us/step - loss: 0.6350 - acc: 0.7551 - val_loss: 0.6443 - val_acc: 0.7550\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 2s 253us/step - loss: 0.6423 - acc: 0.7553 - val_loss: 0.6429 - val_acc: 0.7610\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.6331 - acc: 0.7648 - val_loss: 0.6424 - val_acc: 0.7550\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.6277 - acc: 0.7585 - val_loss: 0.6425 - val_acc: 0.7520\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 2s 250us/step - loss: 0.6271 - acc: 0.7592 - val_loss: 0.6424 - val_acc: 0.7510\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.6316 - acc: 0.7607 - val_loss: 0.6425 - val_acc: 0.7540\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 2s 263us/step - loss: 0.6197 - acc: 0.7647 - val_loss: 0.6405 - val_acc: 0.7530\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 2s 257us/step - loss: 0.6170 - acc: 0.7563 - val_loss: 0.6399 - val_acc: 0.7550\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.6159 - acc: 0.7628 - val_loss: 0.6394 - val_acc: 0.7590\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.6073 - acc: 0.7697 - val_loss: 0.6385 - val_acc: 0.7580\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.6142 - acc: 0.7612 - val_loss: 0.6385 - val_acc: 0.7540\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.6077 - acc: 0.7684 - val_loss: 0.6382 - val_acc: 0.7520\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.6148 - acc: 0.7621 - val_loss: 0.6369 - val_acc: 0.7580\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.6134 - acc: 0.7671 - val_loss: 0.6376 - val_acc: 0.7560\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.6084 - acc: 0.7647 - val_loss: 0.6371 - val_acc: 0.7590\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.5934 - acc: 0.7771 - val_loss: 0.6358 - val_acc: 0.7560\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.6047 - acc: 0.7648 - val_loss: 0.6362 - val_acc: 0.7540\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.5968 - acc: 0.7721 - val_loss: 0.6356 - val_acc: 0.7540\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.6019 - acc: 0.7643 - val_loss: 0.6375 - val_acc: 0.7480\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.6012 - acc: 0.7675 - val_loss: 0.6356 - val_acc: 0.7520\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 2s 249us/step - loss: 0.5901 - acc: 0.7701 - val_loss: 0.6345 - val_acc: 0.7560\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.5912 - acc: 0.7744 - val_loss: 0.6345 - val_acc: 0.7570\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.5929 - acc: 0.7725 - val_loss: 0.6341 - val_acc: 0.7570\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.5845 - acc: 0.7707 - val_loss: 0.6329 - val_acc: 0.7560\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.5853 - acc: 0.7755 - val_loss: 0.6330 - val_acc: 0.7600\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.5855 - acc: 0.7777 - val_loss: 0.6331 - val_acc: 0.7580\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.5799 - acc: 0.7803 - val_loss: 0.6324 - val_acc: 0.7550\n",
      "Epoch 179/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.5833 - acc: 0.7747 - val_loss: 0.6319 - val_acc: 0.7600\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.5835 - acc: 0.7656 - val_loss: 0.6319 - val_acc: 0.7590\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.5849 - acc: 0.7700 - val_loss: 0.6320 - val_acc: 0.7580\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.5791 - acc: 0.7715 - val_loss: 0.6323 - val_acc: 0.7570\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.5721 - acc: 0.7789 - val_loss: 0.6317 - val_acc: 0.7570\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 2s 245us/step - loss: 0.5708 - acc: 0.7771 - val_loss: 0.6311 - val_acc: 0.7560\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.5743 - acc: 0.7741 - val_loss: 0.6314 - val_acc: 0.7580\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.5635 - acc: 0.7761 - val_loss: 0.6304 - val_acc: 0.7580\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 2s 245us/step - loss: 0.5766 - acc: 0.7765 - val_loss: 0.6300 - val_acc: 0.7600\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.5602 - acc: 0.7773 - val_loss: 0.6305 - val_acc: 0.7560\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 2s 250us/step - loss: 0.5635 - acc: 0.7776 - val_loss: 0.6289 - val_acc: 0.7590\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.5614 - acc: 0.7795 - val_loss: 0.6299 - val_acc: 0.7560\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.5649 - acc: 0.7793 - val_loss: 0.6293 - val_acc: 0.7550\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.5543 - acc: 0.7849 - val_loss: 0.6296 - val_acc: 0.7590\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.5552 - acc: 0.7773 - val_loss: 0.6294 - val_acc: 0.7560\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 2s 249us/step - loss: 0.5592 - acc: 0.7843 - val_loss: 0.6294 - val_acc: 0.7590\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.5449 - acc: 0.7885 - val_loss: 0.6292 - val_acc: 0.7570\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.5506 - acc: 0.7824 - val_loss: 0.6281 - val_acc: 0.7600\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.5545 - acc: 0.7860 - val_loss: 0.6288 - val_acc: 0.7610\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.5441 - acc: 0.7852 - val_loss: 0.6288 - val_acc: 0.7600\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.5538 - acc: 0.7821 - val_loss: 0.6283 - val_acc: 0.7600\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.5472 - acc: 0.7799 - val_loss: 0.6287 - val_acc: 0.7600\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the results from `model.evaluate` to see how this change has affected our training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 2s 235us/step\n",
      "1500/1500 [==============================] - 0s 244us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3844145245234172, 0.8393333333333334]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Results: [0.36925017188787462, 0.88026666666666664]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6138144634564717, 0.7546666671435038]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Results: [0.69210424280166627, 0.74333333365122478]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! However, the variance did become higher again, compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.  More Training Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution to high variance is to just get more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets.\n",
    "\n",
    "Run the cell below to preprocess our entire dataset, instead of just working with a subset of the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build the first model that we built, without any regularization or dropout layers included. \n",
    "\n",
    "Train this model for 120 epochs.  All other hyperparameters should stay the same.  Store the fitted model inside of `moredata_model`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 9s 268us/step - loss: 1.9247 - acc: 0.1887 - val_loss: 1.8788 - val_acc: 0.2557\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 8s 237us/step - loss: 1.8282 - acc: 0.3059 - val_loss: 1.7599 - val_acc: 0.3567\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 8s 246us/step - loss: 1.6769 - acc: 0.4045 - val_loss: 1.5805 - val_acc: 0.4603\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 8s 235us/step - loss: 1.4787 - acc: 0.5143 - val_loss: 1.3765 - val_acc: 0.5583\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 8s 237us/step - loss: 1.2810 - acc: 0.6035 - val_loss: 1.1919 - val_acc: 0.6377\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 8s 236us/step - loss: 1.1122 - acc: 0.6558 - val_loss: 1.0449 - val_acc: 0.6777\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 8s 242us/step - loss: 0.9821 - acc: 0.6875 - val_loss: 0.9336 - val_acc: 0.6940\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 9s 263us/step - loss: 0.8867 - acc: 0.7075 - val_loss: 0.8539 - val_acc: 0.7117\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 8s 250us/step - loss: 0.8182 - acc: 0.7209 - val_loss: 0.7962 - val_acc: 0.7253\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 10s 297us/step - loss: 0.7678 - acc: 0.7310 - val_loss: 0.7538 - val_acc: 0.7347\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 10s 314us/step - loss: 0.7296 - acc: 0.7425 - val_loss: 0.7213 - val_acc: 0.7443\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 9s 281us/step - loss: 0.6999 - acc: 0.7477 - val_loss: 0.6962 - val_acc: 0.7507\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 10s 311us/step - loss: 0.6756 - acc: 0.7564 - val_loss: 0.6752 - val_acc: 0.7543\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 9s 264us/step - loss: 0.6557 - acc: 0.7627 - val_loss: 0.6577 - val_acc: 0.7640\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 8s 244us/step - loss: 0.6385 - acc: 0.7697 - val_loss: 0.6438 - val_acc: 0.7697\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 8s 234us/step - loss: 0.6236 - acc: 0.7752 - val_loss: 0.6305 - val_acc: 0.7693\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 8s 234us/step - loss: 0.6102 - acc: 0.7791 - val_loss: 0.6195 - val_acc: 0.7753\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 8s 237us/step - loss: 0.5983 - acc: 0.7830 - val_loss: 0.6106 - val_acc: 0.7783\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 8s 235us/step - loss: 0.5880 - acc: 0.7864 - val_loss: 0.6027 - val_acc: 0.7803\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 8s 236us/step - loss: 0.5784 - acc: 0.7896 - val_loss: 0.5950 - val_acc: 0.7840\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 10s 308us/step - loss: 0.5694 - acc: 0.7935 - val_loss: 0.5888 - val_acc: 0.7840\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 11s 318us/step - loss: 0.5611 - acc: 0.7964 - val_loss: 0.5824 - val_acc: 0.7910\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 8s 240us/step - loss: 0.5532 - acc: 0.7988 - val_loss: 0.5800 - val_acc: 0.7890\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 8s 243us/step - loss: 0.5464 - acc: 0.8003 - val_loss: 0.5724 - val_acc: 0.7907\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 10s 303us/step - loss: 0.5395 - acc: 0.8028 - val_loss: 0.5677 - val_acc: 0.7917\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 9s 287us/step - loss: 0.5332 - acc: 0.8060 - val_loss: 0.5631 - val_acc: 0.7977\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 8s 239us/step - loss: 0.5270 - acc: 0.8086 - val_loss: 0.5621 - val_acc: 0.7980\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 8s 245us/step - loss: 0.5212 - acc: 0.8119 - val_loss: 0.5570 - val_acc: 0.7987\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 9s 264us/step - loss: 0.5155 - acc: 0.8127 - val_loss: 0.5517 - val_acc: 0.8010\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 9s 274us/step - loss: 0.5102 - acc: 0.8154 - val_loss: 0.5516 - val_acc: 0.8053\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 10s 295us/step - loss: 0.5054 - acc: 0.8175 - val_loss: 0.5450 - val_acc: 0.7997\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 10s 294us/step - loss: 0.5006 - acc: 0.8196 - val_loss: 0.5426 - val_acc: 0.8037\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 9s 275us/step - loss: 0.4957 - acc: 0.8225 - val_loss: 0.5401 - val_acc: 0.8053\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 9s 285us/step - loss: 0.4913 - acc: 0.8237 - val_loss: 0.5374 - val_acc: 0.8100\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 10s 309us/step - loss: 0.4868 - acc: 0.8264 - val_loss: 0.5384 - val_acc: 0.8120\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 9s 259us/step - loss: 0.4826 - acc: 0.8273 - val_loss: 0.5333 - val_acc: 0.8083\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 8s 247us/step - loss: 0.4786 - acc: 0.8286 - val_loss: 0.5359 - val_acc: 0.8007\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 8s 257us/step - loss: 0.4749 - acc: 0.8312 - val_loss: 0.5288 - val_acc: 0.8107\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 9s 273us/step - loss: 0.4710 - acc: 0.8319 - val_loss: 0.5286 - val_acc: 0.8127\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 9s 264us/step - loss: 0.4672 - acc: 0.8336 - val_loss: 0.5255 - val_acc: 0.8093\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 8s 250us/step - loss: 0.4636 - acc: 0.8338 - val_loss: 0.5281 - val_acc: 0.8093\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 8s 252us/step - loss: 0.4603 - acc: 0.8359 - val_loss: 0.5225 - val_acc: 0.8110\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 8s 253us/step - loss: 0.4568 - acc: 0.8383 - val_loss: 0.5225 - val_acc: 0.8130\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 9s 274us/step - loss: 0.4536 - acc: 0.8388 - val_loss: 0.5198 - val_acc: 0.8143\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 10s 296us/step - loss: 0.4501 - acc: 0.8404 - val_loss: 0.5183 - val_acc: 0.8153\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 9s 286us/step - loss: 0.4473 - acc: 0.8402 - val_loss: 0.5194 - val_acc: 0.8110\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 9s 287us/step - loss: 0.4443 - acc: 0.8421 - val_loss: 0.5164 - val_acc: 0.8167\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 9s 286us/step - loss: 0.4411 - acc: 0.8435 - val_loss: 0.5171 - val_acc: 0.8167\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 9s 285us/step - loss: 0.4383 - acc: 0.8454 - val_loss: 0.5146 - val_acc: 0.8157\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 9s 287us/step - loss: 0.4354 - acc: 0.8454 - val_loss: 0.5128 - val_acc: 0.8177\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 9s 287us/step - loss: 0.4330 - acc: 0.8465 - val_loss: 0.5124 - val_acc: 0.8167\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 9s 283us/step - loss: 0.4296 - acc: 0.8482 - val_loss: 0.5125 - val_acc: 0.8210\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 9s 283us/step - loss: 0.4274 - acc: 0.8483 - val_loss: 0.5116 - val_acc: 0.8167\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 9s 282us/step - loss: 0.4247 - acc: 0.8497 - val_loss: 0.5127 - val_acc: 0.8177\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 10s 291us/step - loss: 0.4221 - acc: 0.8509 - val_loss: 0.5108 - val_acc: 0.8197\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 9s 284us/step - loss: 0.4194 - acc: 0.8528 - val_loss: 0.5109 - val_acc: 0.8180\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 9s 281us/step - loss: 0.4172 - acc: 0.8537 - val_loss: 0.5095 - val_acc: 0.8187\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 9s 283us/step - loss: 0.4150 - acc: 0.8539 - val_loss: 0.5081 - val_acc: 0.8193\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 9s 268us/step - loss: 0.4123 - acc: 0.8557 - val_loss: 0.5075 - val_acc: 0.8183\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 9s 276us/step - loss: 0.4098 - acc: 0.8558 - val_loss: 0.5093 - val_acc: 0.8183\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 10s 299us/step - loss: 0.4080 - acc: 0.8562 - val_loss: 0.5097 - val_acc: 0.8180\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 9s 277us/step - loss: 0.4059 - acc: 0.8574 - val_loss: 0.5066 - val_acc: 0.8173\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 8s 241us/step - loss: 0.4033 - acc: 0.8594 - val_loss: 0.5065 - val_acc: 0.8187\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 8s 238us/step - loss: 0.4009 - acc: 0.8599 - val_loss: 0.5077 - val_acc: 0.8180\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 8s 230us/step - loss: 0.3992 - acc: 0.8604 - val_loss: 0.5076 - val_acc: 0.8193\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 8s 230us/step - loss: 0.3971 - acc: 0.8613 - val_loss: 0.5064 - val_acc: 0.8193\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 8s 230us/step - loss: 0.3950 - acc: 0.8613 - val_loss: 0.5071 - val_acc: 0.8170\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 8s 235us/step - loss: 0.3930 - acc: 0.8630 - val_loss: 0.5100 - val_acc: 0.8173\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 8s 241us/step - loss: 0.3909 - acc: 0.8641 - val_loss: 0.5064 - val_acc: 0.8180\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 8s 233us/step - loss: 0.3890 - acc: 0.8633 - val_loss: 0.5085 - val_acc: 0.8160\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 8s 237us/step - loss: 0.3871 - acc: 0.8654 - val_loss: 0.5061 - val_acc: 0.8183\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 8s 230us/step - loss: 0.3852 - acc: 0.8663 - val_loss: 0.5088 - val_acc: 0.8187\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 8s 246us/step - loss: 0.3835 - acc: 0.8666 - val_loss: 0.5080 - val_acc: 0.8177\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 8s 243us/step - loss: 0.3814 - acc: 0.8668 - val_loss: 0.5077 - val_acc: 0.8203\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 8s 249us/step - loss: 0.3797 - acc: 0.8675 - val_loss: 0.5075 - val_acc: 0.8160\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 8s 233us/step - loss: 0.3778 - acc: 0.8678 - val_loss: 0.5110 - val_acc: 0.8143\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 8s 229us/step - loss: 0.3761 - acc: 0.8683 - val_loss: 0.5098 - val_acc: 0.8173\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 8s 233us/step - loss: 0.3746 - acc: 0.8695 - val_loss: 0.5078 - val_acc: 0.8177\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 9s 269us/step - loss: 0.3729 - acc: 0.8692 - val_loss: 0.5078 - val_acc: 0.8183\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 10s 314us/step - loss: 0.3709 - acc: 0.8712 - val_loss: 0.5087 - val_acc: 0.8217\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 9s 277us/step - loss: 0.3692 - acc: 0.8709 - val_loss: 0.5100 - val_acc: 0.8213\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 9s 259us/step - loss: 0.3679 - acc: 0.8722 - val_loss: 0.5104 - val_acc: 0.8167\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 8s 250us/step - loss: 0.3665 - acc: 0.8716 - val_loss: 0.5110 - val_acc: 0.8170\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 9s 271us/step - loss: 0.3647 - acc: 0.8729 - val_loss: 0.5150 - val_acc: 0.8173\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 8s 232us/step - loss: 0.3630 - acc: 0.8728 - val_loss: 0.5098 - val_acc: 0.8200\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 8s 230us/step - loss: 0.3612 - acc: 0.8743 - val_loss: 0.5114 - val_acc: 0.8193\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 9s 268us/step - loss: 0.3600 - acc: 0.8747 - val_loss: 0.5105 - val_acc: 0.8203\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 9s 263us/step - loss: 0.3582 - acc: 0.8750 - val_loss: 0.5131 - val_acc: 0.8213\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 8s 230us/step - loss: 0.3566 - acc: 0.8762 - val_loss: 0.5131 - val_acc: 0.8187\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 8s 229us/step - loss: 0.3553 - acc: 0.8762 - val_loss: 0.5120 - val_acc: 0.8213\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 9s 285us/step - loss: 0.3537 - acc: 0.8770 - val_loss: 0.5124 - val_acc: 0.8193\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 9s 286us/step - loss: 0.3523 - acc: 0.8770 - val_loss: 0.5123 - val_acc: 0.8187\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 9s 260us/step - loss: 0.3510 - acc: 0.8782 - val_loss: 0.5143 - val_acc: 0.8160\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 8s 231us/step - loss: 0.3497 - acc: 0.8788 - val_loss: 0.5155 - val_acc: 0.8170\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 8s 243us/step - loss: 0.3481 - acc: 0.8789 - val_loss: 0.5148 - val_acc: 0.8173\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 8s 230us/step - loss: 0.3466 - acc: 0.8796 - val_loss: 0.5168 - val_acc: 0.8210\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 8s 257us/step - loss: 0.3452 - acc: 0.8811 - val_loss: 0.5147 - val_acc: 0.8183\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 10s 298us/step - loss: 0.3438 - acc: 0.8809 - val_loss: 0.5195 - val_acc: 0.8163\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 10s 292us/step - loss: 0.3427 - acc: 0.8807 - val_loss: 0.5179 - val_acc: 0.8180\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 10s 290us/step - loss: 0.3409 - acc: 0.8812 - val_loss: 0.5180 - val_acc: 0.8187\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 9s 284us/step - loss: 0.3396 - acc: 0.8820 - val_loss: 0.5206 - val_acc: 0.8197\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 8s 245us/step - loss: 0.3383 - acc: 0.8832 - val_loss: 0.5178 - val_acc: 0.8177\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 8s 236us/step - loss: 0.3373 - acc: 0.8827 - val_loss: 0.5193 - val_acc: 0.8183\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 8s 230us/step - loss: 0.3359 - acc: 0.8840 - val_loss: 0.5247 - val_acc: 0.8133\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 8s 229us/step - loss: 0.3346 - acc: 0.8840 - val_loss: 0.5238 - val_acc: 0.8167\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 8s 229us/step - loss: 0.3331 - acc: 0.8844 - val_loss: 0.5228 - val_acc: 0.8170\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 8s 237us/step - loss: 0.3323 - acc: 0.8851 - val_loss: 0.5228 - val_acc: 0.8177\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 8s 232us/step - loss: 0.3307 - acc: 0.8858 - val_loss: 0.5247 - val_acc: 0.8167\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 9s 284us/step - loss: 0.3294 - acc: 0.8865 - val_loss: 0.5277 - val_acc: 0.8173\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 9s 260us/step - loss: 0.3282 - acc: 0.8867 - val_loss: 0.5245 - val_acc: 0.8153\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 9s 263us/step - loss: 0.3274 - acc: 0.8858 - val_loss: 0.5260 - val_acc: 0.8160\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 8s 238us/step - loss: 0.3258 - acc: 0.8864 - val_loss: 0.5258 - val_acc: 0.8200\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 8s 244us/step - loss: 0.3245 - acc: 0.8875 - val_loss: 0.5307 - val_acc: 0.8183\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 8s 232us/step - loss: 0.3235 - acc: 0.8883 - val_loss: 0.5288 - val_acc: 0.8177\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 8s 237us/step - loss: 0.3224 - acc: 0.8884 - val_loss: 0.5293 - val_acc: 0.8167\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 8s 241us/step - loss: 0.3212 - acc: 0.8895 - val_loss: 0.5316 - val_acc: 0.8187\n",
      "Epoch 117/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 8s 229us/step - loss: 0.3203 - acc: 0.8896 - val_loss: 0.5301 - val_acc: 0.8167\n",
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 8s 234us/step - loss: 0.3188 - acc: 0.8909 - val_loss: 0.5331 - val_acc: 0.8140\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 9s 269us/step - loss: 0.3173 - acc: 0.8913 - val_loss: 0.5336 - val_acc: 0.8153\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 10s 295us/step - loss: 0.3162 - acc: 0.8904 - val_loss: 0.5339 - val_acc: 0.8190\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finally, let's check the results returned from `model.evaluate()` to see how this model stacks up with the other techniques we've used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 9s 269us/step\n",
      "4000/4000 [==============================] - 1s 290us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3118883702303424, 0.8932727272727272]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Output:  [0.31160746300942971, 0.89160606060606062]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5888206927776337, 0.7995]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Output: [0.56076071488857271, 0.8145]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.1%. Our test set accuracy went up from ~75% to a staggering 81.45% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
